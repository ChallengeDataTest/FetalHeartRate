{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fab4cd",
   "metadata": {},
   "source": [
    "# Le but de ce notebook est d'identifier des caractéristiques très simples permettant d'analyser des battements de coeur de foetus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e01dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# permet de charger les fichiers matlab (*.mat)\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# le répertoire de travail\n",
    "directory = os.path.abspath('')\n",
    "\n",
    "# répertoire où se trouvent toutes les données associés à ce challenge\n",
    "data_directory = os.path.join(directory, 'Data')\n",
    "\n",
    "# fichier CSV contenant les targets (1 / 0)\n",
    "targets_path = os.path.join(data_directory, 'CTG_Challenge_files_GroundTruth.csv')\n",
    "\n",
    "# répertoire où se trouvent les fichiers de données matlab\n",
    "matlab_directory = os.path.join(data_directory, 'ctg_workshop_database')\n",
    "\n",
    "# dans l'électrocardiogramme, nous avons 4 mesures par seconde\n",
    "# chaque mesure correspondent à au nombre de battements de coeurs par minute\n",
    "elements_en_1s = 4\n",
    "elements_en_30minutes = int(elements_en_1s*30*60)\n",
    "elements_en_1heure = int(elements_en_1s*3600)\n",
    "\n",
    "# retourne tous les fichiers matlab présents dans le repertoire 'path'\n",
    "def all_mat_files_in_directory(path: str):\n",
    "    return [os.path.join(path,f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith('.mat')]\n",
    "\n",
    "# calcule la moyenne de la séquence ''fhr' en ignorant les NaN\n",
    "def moyenne(fhr):\n",
    "    return fhr[~np.isnan(fhr)].mean()\n",
    "\n",
    "# calcule la std dev de la séquence ''fhr' en ignorant les NaN\n",
    "def volatilite(fhr):\n",
    "    return fhr[~np.isnan(fhr)].std()\n",
    "\n",
    "# nombre d elements NaN dans la séquence 'fhr'\n",
    "def nan_count(fhr):\n",
    "    return np.count_nonzero(np.isnan(fhr))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7142cf8",
   "metadata": {},
   "source": [
    "# Chargement des données d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d009cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chemin vers tous les fichiers matlab de la base d'entraînement\n",
    "id_to_path = dict()\n",
    "for filename in all_mat_files_in_directory(matlab_directory):\n",
    "    filename_without_extension = os.path.splitext(os.path.basename(filename))[0]    \n",
    "    id = int(filename_without_extension.lstrip('0'))\n",
    "    id_to_path[id] = filename\n",
    "\n",
    "# on charge les targets associés à chaque fichier d'entraînement\n",
    "targets_df = pd.read_csv(targets_path)\n",
    "id_to_target = dict()\n",
    "for _, row in targets_df.iterrows():\n",
    "    id_to_target[row['ChallengeID']] = row['TrueOutcome']\n",
    "\n",
    "# lecture des battements de coeurs des foetus\n",
    "id_to_fhr = dict()\n",
    "all_lengths = []\n",
    "for id, path in id_to_path.items():\n",
    "    matlab_file = loadmat(path)\n",
    "    id_to_fhr[id] = matlab_file['fhr'].flatten()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d15ddac",
   "metadata": {},
   "source": [
    "# Calcul de statistiques sur ces données d'entraînement\n",
    "## (pour faciliter la recherche des caractéristiques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ids = []\n",
    "targets = []\n",
    "\n",
    "moyenne_full = []\n",
    "volatilite_full = []\n",
    "count_full = []\n",
    "nan_count_full = []\n",
    "\n",
    "moyenne_1ere_30minutes = []\n",
    "volatilite_1ere_30minutes = []\n",
    "count_1ere_30minutes = []\n",
    "nan_count_1ere_30minutes = []\n",
    "\n",
    "moyenne_derniere_30minutes = []\n",
    "volatilite_derniere_30minutes = []\n",
    "count_derniere_30minutes = []\n",
    "nan_count_derniere_30minutes = []\n",
    "\n",
    "moyenne_1ere_heure = []\n",
    "volatilite_1ere_heure = []\n",
    "count_1ere_heure = []\n",
    "nan_count_1ere_heure = []\n",
    "\n",
    "moyenne_derniere_heure = []\n",
    "volatilite_derniere_heure = []\n",
    "count_derniere_heure = []\n",
    "nan_count_derniere_heure = []\n",
    "\n",
    "for id in list(id_to_path.keys()):\n",
    "    ids.append(id)\n",
    "    targets.append(id_to_target[id])\n",
    "    \n",
    "    fhr_full = id_to_fhr[id]\n",
    "    \n",
    "    moyenne_full.append(moyenne(fhr_full))\n",
    "    volatilite_full.append(volatilite(fhr_full)) \n",
    "    count_full.append(fhr_full.size)\n",
    "    nan_count_full.append(nan_count(fhr_full))\n",
    "\n",
    "    fhr_1ere_30minutes = fhr_full[:elements_en_30minutes]\n",
    "    moyenne_1ere_30minutes.append(moyenne(fhr_1ere_30minutes))\n",
    "    volatilite_1ere_30minutes.append(volatilite(fhr_1ere_30minutes)) \n",
    "    count_1ere_30minutes.append(fhr_1ere_30minutes.size)\n",
    "    nan_count_1ere_30minutes.append(nan_count(fhr_1ere_30minutes))\n",
    "    \n",
    "    fhr_derniere_30minutes = fhr_full[-elements_en_30minutes:]\n",
    "    moyenne_derniere_30minutes.append(moyenne(fhr_derniere_30minutes))\n",
    "    volatilite_derniere_30minutes.append(volatilite(fhr_derniere_30minutes)) \n",
    "    count_derniere_30minutes.append(fhr_derniere_30minutes.size)\n",
    "    nan_count_derniere_30minutes.append(nan_count(fhr_derniere_30minutes))\n",
    "    \n",
    "    fhr_1ere_heure = fhr_full[:elements_en_1heure]\n",
    "    moyenne_1ere_heure.append(moyenne(fhr_1ere_heure))\n",
    "    volatilite_1ere_heure.append(volatilite(fhr_1ere_heure)) \n",
    "    count_1ere_heure.append(fhr_1ere_heure.size)\n",
    "    nan_count_1ere_heure.append(nan_count(fhr_1ere_heure))\n",
    "    \n",
    "    fhr_derniere_heure = fhr_full[-elements_en_1heure:]\n",
    "    moyenne_derniere_heure.append(moyenne(fhr_derniere_heure))\n",
    "    volatilite_derniere_heure.append(volatilite(fhr_derniere_heure)) \n",
    "    count_derniere_heure.append(fhr_derniere_heure.size)\n",
    "    nan_count_derniere_heure.append(nan_count(fhr_derniere_heure))\n",
    "\n",
    "    \n",
    "# plus petite taille dans le dataset\n",
    "# min([t[~np.isnan(t)].size for t in challengeid_to_fhr.values()])\n",
    " \n",
    "# Sauvegarde de ces statistiques dans un DataFrame\n",
    "fhr_stats = pd.DataFrame(\n",
    "    {'ids': ids,\n",
    "    'targets': targets,\n",
    "    'moyenne_full' : moyenne_full,\n",
    "    'volatilite_full' : volatilite_full,\n",
    "    'count_full' : count_full,\n",
    "    'nan_count_full' : nan_count_full,\n",
    "    'moyenne_1ere_30minutes' : moyenne_1ere_30minutes,\n",
    "    'volatilite_1ere_30minutes' : volatilite_1ere_30minutes,\n",
    "    'count_1ere_30minutes' : count_1ere_30minutes,\n",
    "    'nan_count_1ere_30minutes' : nan_count_1ere_30minutes,\n",
    "    'moyenne_derniere_30minutes' : moyenne_derniere_30minutes,\n",
    "    'volatilite_derniere_30minutes' : volatilite_derniere_30minutes,\n",
    "    'count_derniere_30minutes' : count_derniere_30minutes,\n",
    "    'nan_count_derniere_30minutes' : nan_count_derniere_30minutes,\n",
    "    'moyenne_1ere_heure' : moyenne_1ere_heure,\n",
    "    'volatilite_1ere_heure' : volatilite_1ere_heure,\n",
    "    'count_1ere_heure' : count_1ere_heure,\n",
    "    'nan_count_1ere_heure' : nan_count_1ere_heure,\n",
    "    'moyenne_derniere_heure' : moyenne_derniere_heure,\n",
    "    'volatilite_derniere_heure' : volatilite_derniere_heure,\n",
    "    'count_derniere_heure' : count_derniere_heure,\n",
    "    'nan_count_derniere_heure' : nan_count_derniere_heure,\n",
    "    })\n",
    "\n",
    "# on sauvegarde ces stats sur le disque\n",
    "fhr_stats.to_csv(os.path.join(directory, 'fhr_stats.csv'), index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1bd514",
   "metadata": {},
   "source": [
    "# Affichage d'un électrocardiogramme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d1cbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# id de l'électrocardiogramme à afficher\n",
    "id = 61\n",
    "# nous ne gardons que les 28 dernières minutes de l'enregistrement:\n",
    "# (pour être aligné avec les fichiers pdf du répertoire 'ctg_data')\n",
    "# Il y a 4 mesures par seconde: nous devons donc garder les 4*60*28 dernières valeurs\n",
    "fhr = loadmat(id_to_path[id])['fhr'].ravel()[-1*4*60*28:]\n",
    "\n",
    "# nombre d'éléments dans l'électrocardiogramme\n",
    "n = len(fhr)\n",
    "\n",
    "# Create an array of time points (assuming each heart rate measurement is taken at regular intervals)\n",
    "time = np.arange(n)\n",
    "\n",
    "# Plot the heart rate data\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(time, fhr, linestyle='-', color='black', linewidth=1,label='Fetal Heart Rate')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Heart Rate (bpm)')\n",
    "plt.title('Cardiogram of Heart Rates')\n",
    "plt.legend()\n",
    "plt.xlim(0, n-1)\n",
    "plt.ylim(50, 210)\n",
    "# Display grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495f78b8",
   "metadata": {},
   "source": [
    "# Liste des caractéristiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505aa43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "\n",
    "id_to_data = id_to_fhr\n",
    "\n",
    "\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def pourcentage_points_dans_intervalle(id, valeur_min: float, valeur_max: float) -> float:\n",
    "    fhr = id_to_data[id]\n",
    "    total_points = np.count_nonzero(~np.isnan(fhr))\n",
    "    if total_points<=0:\n",
    "        return 0\n",
    "    total_points_dans_intervalle = np.count_nonzero((fhr > valeur_min) & (fhr < valeur_max) )\n",
    "    return total_points_dans_intervalle/total_points\n",
    "\n",
    "# calcul de la volatilité\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def compute_id_volatility(id: int):\n",
    "    return volatilite(id_to_data[id])\n",
    "   \n",
    "    \n",
    "# calcul de l'etendue de la séquence associée à l'id 'id' en ignorant les NaN\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def compute_id_range(id: int):\n",
    "    data = id_to_data[id]\n",
    "    return max(data)-min(data)\n",
    "   \n",
    "# calcul de la moyenne de la séquence associée à l'id 'id' en ignorant les NaN\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def compute_id_mean(id: int):\n",
    "    return moyenne(id_to_data[id])\n",
    "       \n",
    "# calcul de la moyenne sur l'ensemble du dataset\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def global_mean() -> float :\n",
    "    moyennes = []\n",
    "    for id,data in id_to_data.items():\n",
    "        moyennes.append(compute_id_mean(id)) \n",
    "    return np.mean(moyennes)\n",
    "    \n",
    "# 1ere caracteristique version 1A : sujet sain si le pourcentage de points dans un intervalle donné est supérieur à un seuil (avec ajustement à la moyenne).\n",
    "def calcul_caracteristique1A(id: int, hyperparameters: dict) -> int:\n",
    "    valeur_min_caracteristique1A = hyperparameters['min_value_fhr_caracteristique1A']\n",
    "    valeur_min_caracteristique1A += compute_id_mean(id)-global_mean()\n",
    "    valeur_max_caracteristique1A = valeur_min_caracteristique1A +hyperparameters['range_value_fhr_caracteristique1A']\n",
    "    pourcentage_points_dans_intervalle_caracteristique1A = pourcentage_points_dans_intervalle(id, valeur_min_caracteristique1A, valeur_max_caracteristique1A)\n",
    "    return 0 if (pourcentage_points_dans_intervalle_caracteristique1A > hyperparameters['seuil_caracteristique1A']) else 1\n",
    "\n",
    "# 1ere caracteristique version 1B : sujet sain si le pourcentage de points dans un intervalle donné est supérieur à un seuil (sans ajustement à la moyenne).\n",
    "def calcul_caracteristique1B(id: int, hyperparameters: dict) -> int:\n",
    "    valeur_min_caracteristique1B = hyperparameters['min_value_fhr_caracteristique1B']\n",
    "    valeur_max_caracteristique1B = valeur_min_caracteristique1B +hyperparameters['range_value_fhr_caracteristique1B']\n",
    "    pourcentage_points_dans_intervalle_caracteristique1B = pourcentage_points_dans_intervalle(id, valeur_min_caracteristique1B, valeur_max_caracteristique1B)\n",
    "    return 0 if (pourcentage_points_dans_intervalle_caracteristique1B > hyperparameters['seuil_caracteristique1B']) else 1\n",
    "\n",
    "# 2eme caracteristique version 2A : sujet sain si le pourcentage de points dans un intervalle donné est inférieur à un seuil (avec ajustement à la moyenne).\n",
    "def calcul_caracteristique2A(id: int, hyperparameters: dict) -> int:\n",
    "    valeur_min_caracteristique2A = hyperparameters['min_value_fhr_caracteristique2A']\n",
    "    valeur_min_caracteristique2A += compute_id_mean(id)-global_mean()\n",
    "    valeur_max_caracteristique2A = valeur_min_caracteristique2A +hyperparameters['range_value_fhr_caracteristique2A']\n",
    "    pourcentage_points_dans_intervalle_caracteristique2A = pourcentage_points_dans_intervalle(id, valeur_min_caracteristique2A, valeur_max_caracteristique2A)\n",
    "    return 0 if (pourcentage_points_dans_intervalle_caracteristique2A < hyperparameters['seuil_caracteristique2A']) else 1\n",
    "\n",
    "# 2eme caracteristique version 2B : sujet sain si le pourcentage de points dans un intervalle donné est inférieur à un seuil (sans ajustement à la moyenne).\n",
    "def calcul_caracteristique2B(id: int, hyperparameters: dict) -> int:\n",
    "    valeur_min_caracteristique2B = hyperparameters['min_value_fhr_caracteristique2B']\n",
    "    valeur_max_caracteristique2B = valeur_min_caracteristique2B +hyperparameters['range_value_fhr_caracteristique2B']\n",
    "    pourcentage_points_dans_intervalle_caracteristique2B = pourcentage_points_dans_intervalle(id, valeur_min_caracteristique2B, valeur_max_caracteristique2B)\n",
    "    return 0 if (pourcentage_points_dans_intervalle_caracteristique2B < hyperparameters['seuil_caracteristique2B']) else 1\n",
    "\n",
    "# 3eme caracteristique: sujet sain si l'étendue de l'électrocardiogramme est inférieur à un seuil.\n",
    "def calcul_caracteristique3(id: int, hyperparameters: dict) -> int:\n",
    "    etendue_caracteristique3 = compute_id_range(id)\n",
    "    return 0 if (etendue_caracteristique3 < hyperparameters['seuil_caracteristique3']) else 1\n",
    "\n",
    "# 4eme caracteristique: sujet sain si le pourcentage de points autour de la moyenne est supérieur à un seuil.\n",
    "def calcul_caracteristique4(id: int, hyperparameters: dict) -> int:\n",
    "    range_caracteristique4 = hyperparameters['range_caracteristique4']\n",
    "    moyenne = compute_id_mean(id)\n",
    "    pourcentage_points_dans_intervalle_caracteristique4 = pourcentage_points_dans_intervalle(id, moyenne-range_caracteristique4, moyenne+range_caracteristique4)\n",
    "    return 0 if (pourcentage_points_dans_intervalle_caracteristique4 > hyperparameters['seuil_caracteristique4']) else 1\n",
    "\n",
    "# 5eme caracteristique: sujet sain si la moyenne de l'électrocardiogramme est inférieur à un seuil.\n",
    "def calcul_caracteristique5(id: int, hyperparameters: dict) -> int:\n",
    "    valeur_min_caracteristique5 = hyperparameters['min_value_caracteristique5']\n",
    "    seuil_caracteristique5 = hyperparameters['seuil_caracteristique5']\n",
    "    valeur_max_caracteristique5 = valeur_min_caracteristique5 +seuil_caracteristique5\n",
    "    moyenne_caracteristique5 = compute_id_mean(id)\n",
    "    return 0 if ((moyenne_caracteristique5>=valeur_min_caracteristique5) and (moyenne_caracteristique5<=valeur_max_caracteristique5)) else 1\n",
    "\n",
    "# 6eme caracteristique: sujet sain si le % de points au dessus de la moyenne de l'électrocardiogramme est inférieur à un seuil.\n",
    "def calcul_caracteristique6(id: int, hyperparameters: dict) -> int:\n",
    "    pourcentage_caracteristique6 = hyperparameters['pourcentage_caracteristique6']\n",
    "    moyenne = compute_id_mean(id)\n",
    "    pourcentage_points_au_dessus_de_la_moyenne_caracteristique6 = pourcentage_points_dans_intervalle(id, moyenne, moyenne+9999)\n",
    "    return 0 if (pourcentage_points_au_dessus_de_la_moyenne_caracteristique6 < hyperparameters['pourcentage_caracteristique6']) else 1\n",
    "\n",
    "# 7eme caracteristique: sujet sain si la volatilité de l'électrocardiogramme est inférieur à un seuil.\n",
    "def calcul_caracteristique7(id: int, hyperparameters: dict) -> int:\n",
    "    volatilite_caracteristique7 = compute_id_volatility(id)\n",
    "    return 0 if (volatilite_caracteristique7 < hyperparameters['seuil_caracteristique7']) else 1\n",
    "\n",
    "# 8eme caracteristique: sujet sain si le pourcentage de points dans l'intervalle [ k1 * moyenne, (1+k2) * moyenne] est supérieur à un seuil.\n",
    "def calcul_caracteristique8(id: int, hyperparameters: dict) -> int:\n",
    "    k1_caracteristique8 = hyperparameters['k1_caracteristique8']\n",
    "    k2_caracteristique8 = hyperparameters['k2_caracteristique8']\n",
    "    moyenne = compute_id_mean(id)\n",
    "    pourcentage_points_dans_intervalle_caracteristique8 = pourcentage_points_dans_intervalle(id, k1_caracteristique8*moyenne, (1+k2_caracteristique8)*moyenne)\n",
    "    return 0 if (pourcentage_points_dans_intervalle_caracteristique8 > hyperparameters['seuil_caracteristique8']) else 1\n",
    "\n",
    "# 9eme caracteristique: sujet sain si le pourcentage de points dans l'intervalle [ 0, k] est inférieur à un seuil.\n",
    "def calcul_caracteristique9(id: int, hyperparameters: dict) -> int:\n",
    "    k_caracteristique9 = hyperparameters['k_caracteristique9']\n",
    "    pourcentage_points_dans_intervalle_caracteristique9 = pourcentage_points_dans_intervalle(id, 0, k_caracteristique9)\n",
    "    return 0 if pourcentage_points_dans_intervalle_caracteristique9 < hyperparameters['seuil_caracteristique9'] else 1\n",
    "\n",
    "# 10eme caracteristique: sujet sain si le pourcentage de points dans l'intervalle [ 0, k] est supérieur à un seuil.\n",
    "def calcul_caracteristique10(id: int, hyperparameters: dict) -> int:\n",
    "    k_caracteristique10 = hyperparameters['k_caracteristique10']\n",
    "    pourcentage_points_dans_intervalle_caracteristique10 = pourcentage_points_dans_intervalle(id, 0, k_caracteristique10)\n",
    "    return 0 if pourcentage_points_dans_intervalle_caracteristique10 > hyperparameters['seuil_caracteristique10'] else 1\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6398b39",
   "metadata": {},
   "source": [
    "# Hyperparameters Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04040a0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install hyperopt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from typing import List\n",
    "import math\n",
    "from hyperopt import fmin, tpe, space_eval, hp, Trials, rand as hyperopt_rand\n",
    "import hashlib\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "current_best = 0.5\n",
    "stats_hpo = dict()\n",
    "last_time_display_stats_hpo = time.time()\n",
    "   \n",
    "        \n",
    "def compute_f1_score(TP: int, TN: int, FP: int, FN: int):\n",
    "    return (2*TP)/(1.0*TP+FP+FN)\n",
    "        \n",
    "def compute_accuracy_target0(TP: int, TN: int, FP: int, FN: int):\n",
    "    return TN/(1.0*TN+FP)\n",
    "\n",
    "def compute_accuracy_target1(TP: int, TN: int, FP: int, FN: int):\n",
    "    return TP/(1.0*TP+FN)\n",
    "\n",
    "def compute_score(TP: int, TN: int, FP: int, FN: int):\n",
    "    return compute_average_accuracy(TP,TN,FP,FN)\n",
    "    #return compute_f1_score(TP,TN,FP,FN)\n",
    "    #return min(compute_accuracy_target0(TP,TN,FP,FN),compute_accuracy_target1(TP,TN,FP,FN))\n",
    "\n",
    "def compute_average_accuracy(TP: int, TN: int, FP: int, FN: int):\n",
    "    return (compute_accuracy_target0(TP,TN,FP,FN)+compute_accuracy_target1(TP,TN,FP,FN))/2\n",
    "        \n",
    "def compute_matrice_de_confusion(id_to_predictions, id_to_target) -> (int, int, int,int):\n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        for id, data in id_to_predictions.items():\n",
    "            target = id_to_target[id]\n",
    "            prediction = id_to_predictions[id]\n",
    "            if prediction == target: # bonne prediction\n",
    "                if target == 1:\n",
    "                    TP += 1\n",
    "                else:\n",
    "                    TN += 1\n",
    "            else: # erreur dans la prediciton\n",
    "                if prediction == 1:\n",
    "                    FP += 1\n",
    "                else:\n",
    "                    FN += 1\n",
    "        return (TP,TN,FP,FN)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def compute_single_prediction(id: str, hyperparameters: dict) -> int:  \n",
    "    valeurs_caracteristiques_a_utiliser = []\n",
    "    for c in hyperparameters['caracteristiques_a_utiliser'].split('+'):\n",
    "        # 1ere caracteristique version 1A(avec ajustement à la moyenne)\n",
    "        if c == '1A':\n",
    "            valeurs_caracteristiques_a_utiliser.append(calcul_caracteristique1A(id, hyperparameters))\n",
    "        elif c == '1B':\n",
    "            valeurs_caracteristiques_a_utiliser.append(calcul_caracteristique1B(id, hyperparameters))\n",
    "        elif c == '2A':\n",
    "            valeurs_caracteristiques_a_utiliser.append(calcul_caracteristique2A(id, hyperparameters))\n",
    "        elif c == '2B':\n",
    "            valeurs_caracteristiques_a_utiliser.append(calcul_caracteristique2B(id, hyperparameters))\n",
    "        elif c == '3':\n",
    "            valeurs_caracteristiques_a_utiliser.append(calcul_caracteristique3(id, hyperparameters))\n",
    "        elif c == '4':\n",
    "            valeurs_caracteristiques_a_utiliser.append(calcul_caracteristique4(id, hyperparameters))\n",
    "        elif c == '5':\n",
    "            valeurs_caracteristiques_a_utiliser.append(calcul_caracteristique5(id, hyperparameters))\n",
    "        elif c == '6':\n",
    "            valeurs_caracteristiques_a_utiliser.append(calcul_caracteristique6(id, hyperparameters))\n",
    "        elif c == '7':\n",
    "            valeurs_caracteristiques_a_utiliser.append(calcul_caracteristique7(id, hyperparameters))\n",
    "        elif c == '8':\n",
    "            valeurs_caracteristiques_a_utiliser.append(calcul_caracteristique8(id, hyperparameters))\n",
    "        elif c == '9':\n",
    "            valeurs_caracteristiques_a_utiliser.append(calcul_caracteristique9(id, hyperparameters))\n",
    "        elif c == '10':\n",
    "            valeurs_caracteristiques_a_utiliser.append(calcul_caracteristique10(id, hyperparameters))\n",
    "        else:\n",
    "            raise Exception(f'caracteristique invalide {c}')\n",
    "    if not all_elements_same(valeurs_caracteristiques_a_utiliser):\n",
    "        return hyperparameters['label_si_resultats_differents']\n",
    "    return valeurs_caracteristiques_a_utiliser[0]\n",
    "    \n",
    "    \n",
    "def all_elements_same(data):\n",
    "    first_element = data[0]\n",
    "    return all(element == first_element for element in data)\n",
    "\n",
    "\n",
    "def compute_toutes_les_predictions(hyperparameters: dict) -> dict:\n",
    "    id_to_predictions = dict()\n",
    "    for id in id_to_data.keys():\n",
    "        id_to_predictions[id] = compute_single_prediction(id, hyperparameters)\n",
    "    return id_to_predictions\n",
    "    \n",
    "    \n",
    "    \n",
    "def train(hyperparameters: dict) -> dict:\n",
    "    id_to_predictions = compute_toutes_les_predictions(hyperparameters)\n",
    "    (TP,TN,FP,FN) = compute_matrice_de_confusion(id_to_predictions, id_to_target)\n",
    "    metrics = dict()\n",
    "    metrics['TP'] = TP\n",
    "    metrics['TN'] = TN\n",
    "    metrics['FP'] = FP\n",
    "    metrics['FN'] = FN\n",
    "    metrics['accuracy_target0'] = compute_accuracy_target0(TP,TN,FP,FN)\n",
    "    metrics['accuracy_target1'] = compute_accuracy_target1(TP,TN,FP,FN)\n",
    "    metrics['f1_score'] = compute_f1_score(TP,TN,FP,FN)\n",
    "    metrics['average_accuracy'] = compute_average_accuracy(TP,TN,FP,FN)\n",
    "    current_score = compute_score(TP,TN,FP,FN)\n",
    "    metrics['score'] = current_score\n",
    "    update_stats_hpo(current_score, hyperparameters)\n",
    "    global last_time_display_stats_hpo\n",
    "    if (time.time()-last_time_display_stats_hpo)>600:\n",
    "        save_stats_hpo(False)\n",
    "        last_time_display_stats_hpo = time.time()\n",
    "    global current_best\n",
    "    if not current_best or current_score>current_best:\n",
    "        current_best = current_score\n",
    "        print(f\"new best score {round(current_best,4)} for hyperparameters {[c for c in hyperparameters.items() if c[1] is not None]}\")\n",
    "        save_model(hyperparameters, metrics)\n",
    "        save_stats_hpo(False)\n",
    "    return metrics\n",
    "\n",
    "    \n",
    "def update_stats_hpo(score:float, hyperparameters: dict) -> None:\n",
    "    for hpo_key, hpo_value in hyperparameters.items():\n",
    "        if hpo_value is None:\n",
    "            continue\n",
    "        if hpo_key not in stats_hpo:\n",
    "            stats_hpo[hpo_key] = dict()\n",
    "        if hpo_value not in stats_hpo[hpo_key]:\n",
    "            stats_hpo[hpo_key][hpo_value] = [0,0]\n",
    "        stats_hpo[hpo_key][hpo_value][0] += 1\n",
    "        stats_hpo[hpo_key][hpo_value][1] += score\n",
    "\n",
    "    \n",
    "# the objective used for Hyperparameters Optimization (HPO)\n",
    "# it is the function to minimize\n",
    "def objective(sample_from_search_space):\n",
    "    hyperparameters = fix_hyperparameters(sample_from_search_space)\n",
    "    model_name = get_model_name(hyperparameters)\n",
    "    if model_name in already_processed_model_names_with_hpo:\n",
    "        metrics = already_processed_model_names_with_hpo[model_name]\n",
    "    else:\n",
    "        metrics = train(hyperparameters)\n",
    "        already_processed_model_names_with_hpo[model_name] = metrics\n",
    "    # we want to minimize this objective, so we return -score\n",
    "    return -metrics['score']\n",
    "\n",
    "hpo_session_id = str(int(100*time.time()))\n",
    "\n",
    "def save_stats_hpo(display: bool):\n",
    "    res = stats_hpo_to_str()\n",
    "    try:\n",
    "        path = os.path.join(directory, 'hpo_'+hpo_session_id+\".txt\")\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(res)\n",
    "    except Exception as e:\n",
    "        print(f'failed to save hp')\n",
    "    if display:\n",
    "        print(res)\n",
    "\n",
    "# path to the last model saved\n",
    "last_path_for_save_model = None\n",
    "        \n",
    "def save_model(hyperparameters: dict, metrics: dict) -> None:\n",
    "    global last_path_for_save_model\n",
    "    try:\n",
    "        score = metrics['score']\n",
    "        path = os.path.join(directory, 'hpo_'+hpo_session_id+'_'+hyperparameters['caracteristiques_a_utiliser']+'_'+str(score)+\".txt\")\n",
    "        with open(path, 'w') as f:\n",
    "            text = hyperparameters_to_str(hyperparameters)\n",
    "            for m, v in metrics.items():\n",
    "                text += f'\\n#{m}={v}'\n",
    "            f.write(text)\n",
    "        try:\n",
    "            if last_path_for_save_model and os.path.isfile(last_path_for_save_model):\n",
    "                os.remove(last_path_for_save_model)\n",
    "        except Exception as e:\n",
    "            print(f'failed to delete file {last_path_for_save_model}: {e}')\n",
    "        last_path_for_save_model = path\n",
    "    except Exception as e:\n",
    "        print(f'failed to save hp: {e}')\n",
    "    \n",
    "def stats_hpo_to_str() -> str:\n",
    "    max_intervals = 5\n",
    "    result = \"\"\n",
    "    for hpo_key, hpo_key_stats in stats_hpo.items():\n",
    "        result += f\"stats for key '{hpo_key}':\\n\"\n",
    "        all_values = list(hpo_key_stats.keys())\n",
    "        old_value_to_new_value = dict()\n",
    "        if len(all_values)>max_intervals and (isinstance(all_values[0], int) or isinstance(all_values[0], float)):\n",
    "            min_value = float(min(all_values))\n",
    "            max_value = float(max(all_values))\n",
    "            for v in all_values:\n",
    "                index_interval = int (max_intervals * (float(v-min_value)/(max_value-min_value)))\n",
    "                index_interval = min(index_interval, max_intervals-1)\n",
    "                min_value_interval = min_value+index_interval* (max_value-min_value)/max_intervals\n",
    "                max_value_interval = min_value_interval+ (max_value-min_value)/max_intervals\n",
    "                new_key = f'[{round(min_value_interval,2)}, {round(max_value_interval,2)}]'\n",
    "                if new_key not in old_value_to_new_value:\n",
    "                    old_value_to_new_value[new_key] = [0,0]\n",
    "                old_value_to_new_value[new_key][0] += hpo_key_stats[v][0]\n",
    "                old_value_to_new_value[new_key][1] += hpo_key_stats[v][1]\n",
    "        else:\n",
    "            for v in all_values:\n",
    "                old_value_to_new_value[v] = hpo_key_stats[v]     \n",
    "        \n",
    "        for value, value_stats in sorted(old_value_to_new_value.items(), key=lambda item: item[1][1]/item[1][0], reverse=True):\n",
    "            count = value_stats[0]\n",
    "            avg_score = value_stats[1]/count\n",
    "            result += f'\\t{value} : {avg_score} ({count} samples)\\n'\n",
    "    return result\n",
    "        \n",
    "def extract_caracteristique_id(key: str):\n",
    "    token =\"_caracteristique\"\n",
    "    idx = key.find(token)\n",
    "    if idx<0:\n",
    "        return None\n",
    "    return key[idx+len(token):]\n",
    "\n",
    "# When conducting an HPO search, some hyperparameters may exhibit inconsistent values.\n",
    "# This method aims to address those inconsistencies.\n",
    "def fix_hyperparameters(hyperparameters: dict) -> dict :\n",
    "    res = dict(hyperparameters)\n",
    "    for key in list(res.keys()):\n",
    "        caracteristique_id = extract_caracteristique_id(key)\n",
    "        if caracteristique_id and caracteristique_id not in res['caracteristiques_a_utiliser'].split('+'):\n",
    "            del res[key]\n",
    "    if '+' not in res['caracteristiques_a_utiliser']:\n",
    "        res['label_si_resultats_differents'] = None\n",
    "    return res\n",
    "\n",
    "\n",
    "# Transform the dictionary of hyperparameters 'hyperparameters' into string.\n",
    "def hyperparameters_to_str(hyperparameters: dict) -> str:\n",
    "    sorted_hyperparameters = sorted (hyperparameters.items())\n",
    "    return \"\\n\".join([hyperparameter_name+\" = \"+str(hyperparameter_value) for (hyperparameter_name,hyperparameter_value) in sorted_hyperparameters if hyperparameter_value is not None])\n",
    "\n",
    "def get_model_name(hyperparameters: dict) -> str:\n",
    "    file_content = hyperparameters_to_str(hyperparameters)\n",
    "    return compute_hash(file_content, 10)\n",
    "\n",
    "def compute_hash(input_string, max_length):\n",
    "    # Calculate MD5 hash of the input string\n",
    "    md5_hash = hashlib.md5(input_string.encode('ascii')).hexdigest().upper()\n",
    "    # Return the hash truncated to the max_length\n",
    "    return md5_hash[:max_length]\n",
    "\n",
    "def launch_hpo_for_transformer_model(max_evals: int):\n",
    "    seed = random.randint(0, 100000)\n",
    "    print(f'using seed: {seed}')\n",
    "    rstate = np.random.default_rng(seed)\n",
    "    best_indexes = fmin(\n",
    "        fn=objective,  # \"Loss\" function to minimize\n",
    "        space=search_space,  # Hyperparameter space\n",
    "        #algo=hyperopt_rand.suggest, #Random Search\n",
    "        algo=tpe.suggest,  # Tree-structured Parzen Estimator (TPE)\n",
    "        max_evals=max_evals,  # Perform 'max_evals' trials\n",
    "        max_queue_len = 10,\n",
    "        rstate =rstate,\n",
    "    )\n",
    "\n",
    "    # Get the best parameters\n",
    "    best  = space_eval(search_space, best_indexes)\n",
    "    print(f\"Found minimum after {max_evals} trials:\")\n",
    "    print([c for c in best.items() if c[1] is not None])\n",
    "    return best\n",
    "\n",
    "\n",
    "search_space = {\n",
    "\n",
    "    # 1ere caracteristique version 1A : sujet sain si le pourcentage de points dans un intervalle donné est supérieur à un seuil (avec ajustement à la moyenne).\n",
    "    'min_value_fhr_caracteristique1A': hp.choice('min_value_fhr_caracteristique1A', list(range(50,150+1,1))), # 95 104\n",
    "    'range_value_fhr_caracteristique1A': hp.choice('range_value_fhr_caracteristique1A', list(range(30,150+1,1))), # 125 , 115 , 85 68\n",
    "    'seuil_caracteristique1A': hp.choice('seuil_caracteristique1A', [(i * 0.001) for i in range(800, 1000+1,1)]), # 0.95 0.96 0.91\n",
    "\n",
    "    # 1ere caracteristique version 1B : sujet sain si le pourcentage de points dans un intervalle donné est supérieur à un seuil (sans ajustement à la moyenne).\n",
    "    'min_value_fhr_caracteristique1B': hp.choice('min_value_fhr_caracteristique1B', list(range(50,150+1,1))), # 93\n",
    "    'range_value_fhr_caracteristique1B': hp.choice('range_value_fhr_caracteristique1B', list(range(30,150+1,1))), # 72\n",
    "    'seuil_caracteristique1B': hp.choice('seuil_caracteristique1B', [(i * 0.001) for i in range(800, 1000+1,1)]), # 0.906\n",
    "\n",
    "    # 2eme caracteristique version 2A : sujet sain si le pourcentage de points dans un intervalle donné est inférieur à un seuil (avec ajustement à la moyenne).\n",
    "    'min_value_fhr_caracteristique2A': hp.choice('min_value_fhr_caracteristique2A', list(range(15,50+1,1))),  # 35 , 20, 38 28\n",
    "    'range_value_fhr_caracteristique2A': hp.choice('range_value_fhr_caracteristique2A', list(range(30,100+1,1))), # 65 , 54 64\n",
    "    'seuil_caracteristique2A': hp.choice('seuil_caracteristique2A', [(i * 0.001) for i in range(0, 100+1,1)]), # 0.04 , 0.03 0.01  003\n",
    "\n",
    "    # 2eme caracteristique version 2B : sujet sain si le pourcentage de points dans un intervalle donné est inférieur à un seuil (sans ajustement à la moyenne).\n",
    "    'min_value_fhr_caracteristique2B': hp.choice('min_value_fhr_caracteristique2B', list(range(10,100+1,2))), # 14\n",
    "    'range_value_fhr_caracteristique2B': hp.choice('range_value_fhr_caracteristique2B', list(range(20,120+1,5))), # 70\n",
    "    'seuil_caracteristique2B': hp.choice('seuil_caracteristique2B', [(i * 0.001) for i in range(0, 100+1,1)]), # 0.03\n",
    "\n",
    "    # 3eme caracteristique: sujet sain si l'étendue de l'électrocardiogramme est inférieur à un seuil.\n",
    "    'seuil_caracteristique3': hp.choice('seuil_caracteristique3', list(range(100,150+1,1))), # 124\n",
    "\n",
    "    # 4eme caracteristique: sujet sain si le pourcentage de points autour de la moyenne est supérieur à un seuil.\n",
    "    'range_caracteristique4': hp.choice('range_caracteristique4', list(range(20,75+1,1))), # 43\n",
    "    'seuil_caracteristique4': hp.choice('seuil_caracteristique4', [(i * 0.001) for i in range(900, 1000+1,1)]), # 0.96\n",
    "\n",
    "    # 5eme caracteristique: sujet sain si la moyenne de l'électrocardiogramme est dans l'intervalle [k1, k2]\n",
    "    'min_value_caracteristique5': hp.choice('min_value_caracteristique5', list(range(122-40,122+40+1,1))),\n",
    "    'seuil_caracteristique5': hp.choice('seuil_caracteristique5', list(range(9-6,9+6+1,1))),\n",
    "\n",
    "    # 6eme caracteristique: sujet sain si le % de points au dessus de la moyenne de l'électrocardiogramme est inférieur à un seuil.\n",
    "    'pourcentage_caracteristique6': hp.choice('pourcentage_caracteristique6', [(i * 0.01) for i in range(61-30, 61+30+1,1)]),\n",
    "\n",
    "    # 7eme caracteristique: sujet sain si la volatilité de l'électrocardiogramme est inférieur à un seuil.\n",
    "    'seuil_caracteristique7': hp.choice('seuil_caracteristique7', [(i * 0.01) for i in range(1880, 1980+1,1)]),\n",
    "\n",
    "    # 8eme caracteristique: sujet sain si le pourcentage de points dans l'intervalle [ k1 * moyenne, (1+k2) * moyenne] est supérieur à un seuil.\n",
    "    'k1_caracteristique8': hp.choice('k1_caracteristique8', [(i * 0.01) for i in range(0, 100+1,1)]), # 0.26 0.45 0.72  0.92 0.44\n",
    "    'k2_caracteristique8': hp.choice('k2_caracteristique8', [(i * 0.01) for i in range(0, 50+1,1)]),  # 0.13 0.33 0.1\n",
    "    'seuil_caracteristique8': hp.choice('seuil_caracteristique8', [(i * 0.01) for i in range(70, 100+1,1)]), #0.87 0.96\n",
    "\n",
    "    # 9eme caracteristique: sujet sain si le pourcentage de points dans l'intervalle [ 0, k] est inférieur à un seuil.\n",
    "    'k_caracteristique9': hp.choice('k_caracteristique9', list(range(0, 150+1,1))), # 94\n",
    "    'seuil_caracteristique9': hp.choice('seuil_caracteristique9', [(i * 0.001) for i in range(0, 200+1,1)]), # 0.03\n",
    "\n",
    "    # 10eme caracteristique: sujet sain si le pourcentage de points dans l'intervalle [ 0, k] est supérieur à un seuil.\n",
    "    'k_caracteristique10': hp.choice('k_caracteristique10', list(range(100,250+1,1))), # 156\n",
    "    'seuil_caracteristique10': hp.choice('seuil_caracteristique10', [(i * 0.001) for i in range(800, 1000+1,1)]), # 0.94\n",
    "\n",
    "    'caracteristiques_a_utiliser': hp.choice('caracteristiques_a_utiliser', ['1A+7']),\n",
    "    'label_si_resultats_differents': hp.choice('label_si_resultats_differents', [0, 1]),\n",
    "}\n",
    "\n",
    "\n",
    "if len(sys.argv) >=2 and str.isdigit(sys.argv[1][0]):\n",
    "    caracteristiques_a_utiliser = sys.argv[1].split(',')\n",
    "    print(f'caracteristiques_a_utiliser will be set to {caracteristiques_a_utiliser}')\n",
    "    search_space['caracteristiques_a_utiliser'] =  hp.choice('caracteristiques_a_utiliser', caracteristiques_a_utiliser)\n",
    "\n",
    "\n",
    "max_evals = 100\n",
    "if len(sys.argv) >=3 and str.isdigit(sys.argv[2]):\n",
    "    print(f'max_evals will be set to {sys.argv[2]}')\n",
    "    max_evals = int(sys.argv[2])\n",
    "print(f'max_evals value is {max_evals}')\n",
    "\n",
    "# Uncomment following line to enable HPO\n",
    "already_processed_model_names_with_hpo = dict()\n",
    "start_time = time.time()\n",
    "best = launch_hpo_for_transformer_model(max_evals)\n",
    "print(f'hpo took {round(time.time()-start_time,2)}s')\n",
    "save_stats_hpo(True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
