{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fab4cd",
   "metadata": {},
   "source": [
    "# Le but de ce notebook est d'identifier des caractéristiques très simples permettant d'analyser des battements de coeur de foetus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e01dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install hyperopt\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# permet de charger les fichiers matlab (*.mat)\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# le répertoire de travail\n",
    "directory = os.path.abspath('')\n",
    "\n",
    "# répertoire où se trouvent toutes les données associés à ce challenge\n",
    "data_directory = os.path.join(directory, 'Data')\n",
    "\n",
    "# fichier CSV contenant les targets (1 / 0)\n",
    "targets_path = os.path.join(data_directory, 'CTG_Challenge_files_GroundTruth.csv')\n",
    "\n",
    "# répertoire où se trouvent les fichiers de données matlab\n",
    "matlab_directory = os.path.join(data_directory, 'ctg_workshop_database')\n",
    "\n",
    "# dans l'électrocardiogramme, nous avons 4 mesures par seconde\n",
    "# chaque mesure correspondent à au nombre de battements de coeurs par minute\n",
    "elements_en_1s = 4\n",
    "elements_en_30minutes = int(elements_en_1s*30*60)\n",
    "elements_en_1heure = int(elements_en_1s*3600)\n",
    "\n",
    "# retourne tous les fichiers matlab présents dans le repertoire 'path'\n",
    "def all_mat_files_in_directory(path: str):\n",
    "    return [os.path.join(path,f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith('.mat')]\n",
    "\n",
    "# calcule la moyenne de la séquence ''fhr' en ignorant les NaN\n",
    "def moyenne(fhr):\n",
    "    return fhr[~np.isnan(fhr)].mean()\n",
    "\n",
    "# calcule la médiane de la séquence ''fhr' en ignorant les NaN\n",
    "def mediane(fhr):\n",
    "    return np.nanmedian(fhr)\n",
    "\n",
    "# calcule la std dev de la séquence ''fhr' en ignorant les NaN\n",
    "def volatilite(fhr):\n",
    "    return fhr[~np.isnan(fhr)].std()\n",
    "\n",
    "# nombre d elements NaN dans la séquence 'fhr'\n",
    "def nan_count(fhr):\n",
    "    return np.count_nonzero(np.isnan(fhr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7142cf8",
   "metadata": {},
   "source": [
    "# Chargement des données d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d009cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chemin vers tous les fichiers matlab de la base d'entraînement\n",
    "id_to_path = dict()\n",
    "for filename in all_mat_files_in_directory(matlab_directory):\n",
    "    filename_without_extension = os.path.splitext(os.path.basename(filename))[0]    \n",
    "    id = int(filename_without_extension.lstrip('0'))\n",
    "    id_to_path[id] = filename\n",
    "\n",
    "# on charge les targets associés à chaque fichier d'entraînement\n",
    "targets_df = pd.read_csv(targets_path)\n",
    "id_to_target = dict()\n",
    "for _, row in targets_df.iterrows():\n",
    "    id_to_target[row['ChallengeID']] = row['TrueOutcome']\n",
    "\n",
    "# lecture des battements de coeurs des foetus\n",
    "id_to_fhr = dict()\n",
    "all_lengths = []\n",
    "for id, path in id_to_path.items():\n",
    "    matlab_file = loadmat(path)\n",
    "    id_to_fhr[id] = matlab_file['fhr'].flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d15ddac",
   "metadata": {},
   "source": [
    "# Calcul de statistiques sur ces données d'entraînement\n",
    "## (pour faciliter la recherche des caractéristiques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "targets = []\n",
    "moyenne_full = []\n",
    "mediane_full = []\n",
    "volatilite_full = []\n",
    "count_full = []\n",
    "nan_count_full = []\n",
    "moyenne_1ere_heure = []\n",
    "mediane_1ere_heure = []\n",
    "volatilite_1ere_heure = []\n",
    "count_1ere_heure = []\n",
    "nan_count_1ere_heure = []\n",
    "moyenne_derniere_heure = []\n",
    "mediane_derniere_heure = []\n",
    "volatilite_derniere_heure = []\n",
    "count_derniere_heure = []\n",
    "nan_count_derniere_heure = []\n",
    "\n",
    "for id in list(id_to_path.keys()):\n",
    "    ids.append(id)\n",
    "    targets.append(id_to_target[id])\n",
    "    \n",
    "    fhr_full = id_to_fhr[id]\n",
    "    \n",
    "    moyenne_full.append(moyenne(fhr_full))\n",
    "    mediane_full.append(mediane(fhr_full))\n",
    "    volatilite_full.append(volatilite(fhr_full)) \n",
    "    count_full.append(fhr_full.size)\n",
    "    nan_count_full.append(nan_count(fhr_full))\n",
    "\n",
    "    fhr_1ere_heure = fhr_full[:elements_en_1heure]\n",
    "    moyenne_1ere_heure.append(moyenne(fhr_1ere_heure))\n",
    "    mediane_1ere_heure.append(mediane(fhr_1ere_heure))\n",
    "    volatilite_1ere_heure.append(volatilite(fhr_1ere_heure)) \n",
    "    count_1ere_heure.append(fhr_1ere_heure.size)\n",
    "    nan_count_1ere_heure.append(nan_count(fhr_1ere_heure))\n",
    "    \n",
    "    fhr_derniere_heure = fhr_full[-elements_en_1heure:]\n",
    "    moyenne_derniere_heure.append(moyenne(fhr_derniere_heure))\n",
    "    mediane_derniere_heure.append(mediane(fhr_derniere_heure))\n",
    "    volatilite_derniere_heure.append(volatilite(fhr_derniere_heure)) \n",
    "    count_derniere_heure.append(fhr_derniere_heure.size)\n",
    "    nan_count_derniere_heure.append(nan_count(fhr_derniere_heure))\n",
    "\n",
    "    \n",
    "# Sauvegarde de ces statistiques dans un DataFrame\n",
    "fhr_stats = pd.DataFrame(\n",
    "    {'ids': ids,\n",
    "    'targets': targets,\n",
    "    'moyenne_full' : moyenne_full,\n",
    "    'mediane_full' : mediane_full,\n",
    "    'volatilite_full' : volatilite_full,\n",
    "    'count_full' : count_full,\n",
    "    'nan_count_full' : nan_count_full,\n",
    "    'moyenne_1ere_heure' : moyenne_1ere_heure,\n",
    "    'mediane_1ere_heure' : mediane_1ere_heure,\n",
    "    'volatilite_1ere_heure' : volatilite_1ere_heure,\n",
    "    'count_1ere_heure' : count_1ere_heure,\n",
    "    'nan_count_1ere_heure' : nan_count_1ere_heure,\n",
    "    'moyenne_derniere_heure' : moyenne_derniere_heure,\n",
    "    'mediane_derniere_heure' : mediane_derniere_heure,\n",
    "    'volatilite_derniere_heure' : volatilite_derniere_heure,\n",
    "    'count_derniere_heure' : count_derniere_heure,\n",
    "    'nan_count_derniere_heure' : nan_count_derniere_heure,\n",
    "    })\n",
    "\n",
    "# on sauvegarde ces stats sur le disque\n",
    "fhr_stats.to_csv(os.path.join(directory, 'fhr_stats.csv'), index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1bd514",
   "metadata": {},
   "source": [
    "# Affichage d'un électrocardiogramme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d1cbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# id de l'électrocardiogramme à afficher\n",
    "id = 61\n",
    "# nous ne gardons que les 28 dernières minutes de l'enregistrement:\n",
    "# (pour être aligné avec les fichiers pdf du répertoire 'ctg_data')\n",
    "# Il y a 4 mesures par seconde: nous devons donc garder les 4*60*28 dernières valeurs\n",
    "fhr = loadmat(id_to_path[id])['fhr'].ravel()[-1*4*60*28:]\n",
    "\n",
    "# nombre d'éléments dans l'électrocardiogramme\n",
    "n = len(fhr)\n",
    "\n",
    "# Create an array of time points (assuming each heart rate measurement is taken at regular intervals)\n",
    "time = np.arange(n)\n",
    "\n",
    "# Plot the heart rate data\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(time, fhr, linestyle='-', color='black', linewidth=1,label='Fetal Heart Rate')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Heart Rate (bpm)')\n",
    "plt.title('Cardiogram of Heart Rates')\n",
    "plt.legend()\n",
    "plt.xlim(0, n-1)\n",
    "plt.ylim(50, 210)\n",
    "# Display grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495f78b8",
   "metadata": {},
   "source": [
    "# Liste des caractéristiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505aa43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "id_to_data = id_to_fhr\n",
    "\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def proportion_in_interval(id, min_value: float, max_value: float) -> float:\n",
    "    fhr = id_to_data[id]\n",
    "    total_points = np.count_nonzero(~np.isnan(fhr))\n",
    "    if total_points<=0:\n",
    "        return 0\n",
    "    total_points_in_interval = np.count_nonzero((fhr > min_value) & (fhr < max_value) )\n",
    "    return total_points_in_interval/total_points\n",
    "\n",
    "# calcul de la volatilité\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def compute_id_volatility(id: int):\n",
    "    return volatilite(id_to_data[id])\n",
    "    \n",
    "# calcul de l'etendue de la séquence associée à l'id 'id' en ignorant les NaN\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def compute_id_range(id: int):\n",
    "    data = id_to_data[id]\n",
    "    return max(data)-min(data)\n",
    "   \n",
    "# calcul de la moyenne de la séquence associée à l'id 'id' en ignorant les NaN\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def compute_id_mean(id: int):\n",
    "    return moyenne(id_to_data[id])\n",
    "\n",
    "# calcul de la moyenne sur l'ensemble du dataset\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def global_mean() -> float :\n",
    "    moyennes = []\n",
    "    for id,data in id_to_data.items():\n",
    "        moyennes.append(compute_id_mean(id)) \n",
    "    return np.mean(moyennes)\n",
    "\n",
    "# calcul de la médiane de la séquence associée à l'id 'id' en ignorant les NaN\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def compute_id_median(id: int):\n",
    "    return mediane(id_to_data[id])\n",
    "\n",
    "# calcul de l'écart type autour de la médiane de la séquence associée à l'id 'id' en ignorant les NaN\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def compute_id_std_median(id: int) -> float:\n",
    "    data = id_to_data[id]\n",
    "    median = compute_id_median(id)\n",
    "    deviations = data - median\n",
    "    squared_deviations = deviations ** 2\n",
    "    variance = np.nanmean(squared_deviations)\n",
    "    std_deviation = np.sqrt(variance)\n",
    "    return std_deviation\n",
    "\n",
    "# calcul de la médiane sur l'ensemble du dataset\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def global_median() -> float :\n",
    "    medianes = []\n",
    "    for id,data in id_to_data.items():\n",
    "        medianes.append(compute_id_median(id)) \n",
    "    return np.mean(medianes)\n",
    "\n",
    "# calcul du % de fois où une valeur a été passée\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def percentage_crossings(id: int, target_value: int) -> float:\n",
    "    series = id_to_data[id]\n",
    "    # Remove NaN values\n",
    "    valid_indices = ~np.isnan(series)\n",
    "    series = series[valid_indices]\n",
    "    if len(series) == 0:\n",
    "        return 0.0\n",
    "    # Create a boolean array where True indicates the value is greater than the specified value\n",
    "    above_value = series > target_value\n",
    "    # Calculate the difference of the boolean array\n",
    "    # A change from False to True or True to False indicates a crossing\n",
    "    crossings = np.diff(above_value.astype(int))\n",
    "    # Count the number of crossings\n",
    "    num_crossings = np.sum(np.abs(crossings))\n",
    "    # Return the % of crossing\n",
    "    return num_crossings/len(series)\n",
    "\n",
    "\n",
    "\n",
    "# 1ere caracteristique version 1A : sujet sain si le pourcentage de points dans un intervalle donné est supérieur à un seuil (avec ajustement à la moyenne).\n",
    "def calcul_caracteristique1A(id: int, hyperparameters: dict, suffix: str) -> int:\n",
    "    min_value = hyperparameters['min_value_'+suffix]\n",
    "    min_value += compute_id_mean(id)-global_mean()\n",
    "    max_value = min_value +hyperparameters['range_'+suffix]\n",
    "    return 0 if proportion_in_interval(id, min_value, max_value) > hyperparameters['seuil_'+suffix] else 1\n",
    "\n",
    "# 1ere caracteristique version 1B : sujet sain si le pourcentage de points dans un intervalle donné est supérieur à un seuil (sans ajustement à la moyenne).\n",
    "def calcul_caracteristique1B(id: int, hyperparameters: dict, suffix: str) -> int:\n",
    "    min_value = hyperparameters['min_value_'+suffix]\n",
    "    max_value = min_value  +hyperparameters['range_'+suffix]\n",
    "    return 0 if proportion_in_interval(id, min_value, max_value) > hyperparameters['seuil_'+suffix] else 1\n",
    "\n",
    "# 3eme caracteristique: sujet sain si l'étendue de l'électrocardiogramme est inférieur à un seuil.\n",
    "def calcul_caracteristique3(id: int, hyperparameters: dict, suffix: str) -> int:\n",
    "    return 0 if compute_id_range(id)< hyperparameters['seuil_'+suffix] else 1\n",
    "\n",
    "# 4eme caracteristique: sujet sain si le pourcentage de points autour de la moyenne est supérieur à un seuil.\n",
    "def calcul_caracteristique4A(id: int, hyperparameters: dict, suffix: str) -> int:\n",
    "    range = hyperparameters['range_'+suffix]\n",
    "    moyenne = compute_id_mean(id)\n",
    "    return 0 if proportion_in_interval(id, moyenne-range, moyenne+range) > hyperparameters['seuil_'+suffix] else 1\n",
    "\n",
    "# caracteristique 4B: sujet sain si le pourcentage de points autour de la médiane est supérieur à un seuil.\n",
    "def calcul_caracteristique4B(id: int, hyperparameters: dict, suffix: str) -> int:\n",
    "    range = hyperparameters['range_'+suffix]\n",
    "    median = compute_id_median(id)\n",
    "    return 0 if proportion_in_interval(id, median-range, median+range) > hyperparameters['seuil_'+suffix] else 1\n",
    "\n",
    "# caracteristique 5A: sujet sain si la moyenne de l'électrocardiogramme est dans un intervalle donné\n",
    "def calcul_caracteristique5A(id: int, hyperparameters: dict, suffix: str) -> int:\n",
    "    min_value = hyperparameters['min_value_'+suffix]\n",
    "    max_value = min_value + hyperparameters['range_'+suffix]\n",
    "    moyenne = compute_id_mean(id)\n",
    "    return 0 if ((moyenne>=min_value) and (moyenne<=max_value)) else 1\n",
    "    \n",
    "# caracteristique 5B: sujet sain si la médiane de l'électrocardiogramme est dans un intervalle donné\n",
    "def calcul_caracteristique5B(id: int, hyperparameters: dict, suffix: str) -> int:\n",
    "    min_value = hyperparameters['min_value_'+suffix]\n",
    "    max_value = min_value + hyperparameters['range_'+suffix]\n",
    "    median = compute_id_median(id)\n",
    "    return 0 if ((median>=min_value) and (median<=max_value)) else 1\n",
    "\n",
    "# caracteristique 6: sujet sain si le % de points au dessus de la moyenne de l'électrocardiogramme est inférieur à un seuil.\n",
    "def calcul_caracteristique6(id: int, hyperparameters: dict, suffix: str) -> int:\n",
    "    moyenne = compute_id_mean(id)\n",
    "    return 0 if proportion_in_interval(id, moyenne, 1e9) < hyperparameters['seuil_'+suffix] else 1\n",
    "\n",
    "# 7eme caracteristique: sujet sain si la volatilité de l'électrocardiogramme est inférieur à un seuil.\n",
    "def calcul_caracteristique7A(id: int, hyperparameters: dict, suffix: str) -> int:\n",
    "    return 0 if compute_id_volatility(id) < hyperparameters['seuil_'+suffix] else 1\n",
    "\n",
    "# caracteristique 7B: sujet sain si l'écart type autour de la médiane de l'électrocardiogramme est inférieur à un seuil.\n",
    "def calcul_caracteristique7B(id: int, hyperparameters: dict, suffix: str) -> int:\n",
    "    return 0 if compute_id_std_median(id) < hyperparameters['seuil_'+suffix] else 1\n",
    "\n",
    "# 8eme caracteristique: sujet sain si le pourcentage de points dans l'intervalle [ k1 * moyenne, (1+k2) * moyenne] est supérieur à un seuil.\n",
    "def calcul_caracteristique8(id: int, hyperparameters: dict, suffix: str) -> int:\n",
    "    k1 = hyperparameters['k1_'+suffix]\n",
    "    k2 = hyperparameters['k2_'+suffix]\n",
    "    moyenne = compute_id_mean(id)\n",
    "    return 0 if proportion_in_interval(id, k1*moyenne, (1+k2)*moyenne) > hyperparameters['seuil_'+suffix] else 1\n",
    "\n",
    "# caracteristique 9: sujet sain si le pourcentage de points dans l'intervalle [ 0, k] est inférieur à un seuil.\n",
    "def calcul_caracteristique9(id: int, hyperparameters: dict, suffix: str) -> int:\n",
    "    k = hyperparameters['k_'+suffix]\n",
    "    return 0 if proportion_in_interval(id, 0, k) < hyperparameters['seuil_'+suffix] else 1\n",
    "   \n",
    "def calcul_caracteristique13(id: int, hyperparameters: dict, suffix: str) -> int:\n",
    "    k = hyperparameters['k_'+suffix]\n",
    "    return 0 if percentage_crossings(id, k) > hyperparameters['seuil_'+suffix] else 1\n",
    "\n",
    "def calcul_caracteristique14A(id: int, hyperparameters: dict, suffix: str) -> int:\n",
    "    return 0 if percentage_crossings(id, compute_id_mean(id)) > hyperparameters['seuil_'+suffix] else 1\n",
    "\n",
    "def calcul_caracteristique14B(id: int, hyperparameters: dict, suffix: str) -> int:\n",
    "    return 0 if percentage_crossings(id, compute_id_median(id)) > hyperparameters['seuil_'+suffix] else 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6398b39",
   "metadata": {},
   "source": [
    "# Hyperparameters Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04040a0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from typing import List\n",
    "import math\n",
    "from hyperopt import fmin, tpe, space_eval, hp, Trials, rand as hyperopt_rand\n",
    "import hashlib\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "\n",
    "current_best = None\n",
    "stats_hpo = dict()\n",
    "last_time_display_stats_hpo = time.time()\n",
    "   \n",
    "def compute_score(TP: int, TN: int, FP: int, FN: int):\n",
    "    return compute_average_accuracy(TP,TN,FP,FN)\n",
    "        \n",
    "def compute_f1_score(TP: int, TN: int, FP: int, FN: int):\n",
    "    return (2*TP)/(2*TP+FP+FN)\n",
    "        \n",
    "def compute_accuracy_target0(TP: int, TN: int, FP: int, FN: int):\n",
    "    return TN/(1*TN+FP)\n",
    "\n",
    "def compute_accuracy_target1(TP: int, TN: int, FP: int, FN: int):\n",
    "    return TP/(1*TP+FN)\n",
    "\n",
    "def compute_average_accuracy(TP: int, TN: int, FP: int, FN: int):\n",
    "    return (compute_accuracy_target0(TP,TN,FP,FN)+compute_accuracy_target1(TP,TN,FP,FN))/2\n",
    "        \n",
    "def compute_matrice_de_confusion(id_to_predictions, id_to_target) -> (int, int, int,int):\n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        for id, data in id_to_predictions.items():\n",
    "            target = id_to_target[id]\n",
    "            prediction = id_to_predictions[id]\n",
    "            if prediction == target: # bonne prediction\n",
    "                if target == 1:\n",
    "                    TP += 1\n",
    "                else:\n",
    "                    TN += 1\n",
    "            else: # erreur dans la prediciton\n",
    "                if prediction == 1:\n",
    "                    FP += 1\n",
    "                else:\n",
    "                    FN += 1\n",
    "        return (TP,TN,FP,FN)\n",
    "\n",
    "def compute_single_prediction_single_caracteristique(id: str, hyperparameters: dict, caracteristique: str) -> int:  \n",
    "    if caracteristique[-1:] == 'i':\n",
    "        # we compute the complement of caracteristique 'caracteristique[:-1]'\n",
    "        # ex: caracteristique '10i' will compute the complement of caracteristique '10'\n",
    "        return 1-globals()['calcul_caracteristique'+caracteristique[:-1]](id, hyperparameters, caracteristique)\n",
    "    return globals()['calcul_caracteristique'+caracteristique](id, hyperparameters, caracteristique)\n",
    "\n",
    "def compute_single_prediction(id: str, hyperparameters: dict) -> int:  \n",
    "    valeurs_caracteristiques_a_utiliser = []\n",
    "    for c in hyperparameters['caracteristiques_a_utiliser'].split('+'):\n",
    "        valeurs_caracteristiques_a_utiliser.append(compute_single_prediction_single_caracteristique(id, hyperparameters, c))\n",
    "    if not all_elements_same(valeurs_caracteristiques_a_utiliser):\n",
    "        return hyperparameters['label_si_resultats_differents']\n",
    "    return valeurs_caracteristiques_a_utiliser[0]\n",
    "    \n",
    "def all_elements_same(data):\n",
    "    first_element = data[0]\n",
    "    return all(element == first_element for element in data)\n",
    "\n",
    "def compute_toutes_les_predictions(hyperparameters: dict) -> dict:\n",
    "    id_to_predictions = dict()\n",
    "    for id in id_to_data.keys():\n",
    "        id_to_predictions[id] = compute_single_prediction(id, hyperparameters)\n",
    "    return id_to_predictions\n",
    "    \n",
    "def train(hyperparameters: dict) -> dict:\n",
    "    id_to_predictions = compute_toutes_les_predictions(hyperparameters)\n",
    "    (TP,TN,FP,FN) = compute_matrice_de_confusion(id_to_predictions, id_to_target)\n",
    "    metrics = dict()\n",
    "    metrics['TP'] = TP\n",
    "    metrics['TN'] = TN\n",
    "    metrics['FP'] = FP\n",
    "    metrics['FN'] = FN\n",
    "    metrics['accuracy_target0'] = compute_accuracy_target0(TP,TN,FP,FN)\n",
    "    metrics['accuracy_target1'] = compute_accuracy_target1(TP,TN,FP,FN)\n",
    "    metrics['average_accuracy'] = compute_average_accuracy(TP,TN,FP,FN)\n",
    "    current_score = compute_score(TP,TN,FP,FN)\n",
    "    metrics['score'] = current_score\n",
    "    update_stats_hpo(current_score, hyperparameters)\n",
    "    global last_time_display_stats_hpo\n",
    "    if (time.time()-last_time_display_stats_hpo)>600:\n",
    "        save_stats_hpo(False)\n",
    "        last_time_display_stats_hpo = time.time()\n",
    "    global current_best\n",
    "    if not current_best or current_score>current_best:\n",
    "        current_best = current_score\n",
    "        print(f\"new best score {round(current_best,4)} for hyperparameters {[c for c in hyperparameters.items() if c[1] is not None]}\")\n",
    "        save_model(hyperparameters, metrics)\n",
    "        save_stats_hpo(False)\n",
    "    return metrics\n",
    "\n",
    "    \n",
    "def update_stats_hpo(score:float, hyperparameters: dict) -> None:\n",
    "    for hpo_key, hpo_value in hyperparameters.items():\n",
    "        if hpo_value is None:\n",
    "            continue\n",
    "        if hpo_key not in stats_hpo:\n",
    "            stats_hpo[hpo_key] = dict()\n",
    "        if hpo_value not in stats_hpo[hpo_key]:\n",
    "            stats_hpo[hpo_key][hpo_value] = [0,0]\n",
    "        stats_hpo[hpo_key][hpo_value][0] += 1\n",
    "        stats_hpo[hpo_key][hpo_value][1] += score\n",
    "\n",
    "    \n",
    "# the objective used for Hyperparameters Optimization (HPO)\n",
    "# it is the function to minimize\n",
    "def objective(sample_from_search_space):\n",
    "    hyperparameters = fix_hyperparameters(sample_from_search_space)\n",
    "    model_name = get_model_name(hyperparameters)\n",
    "    if model_name in already_processed_model_names_with_hpo:\n",
    "        metrics = already_processed_model_names_with_hpo[model_name]\n",
    "    else:\n",
    "        metrics = train(hyperparameters)\n",
    "        already_processed_model_names_with_hpo[model_name] = metrics\n",
    "    # we want to minimize this objective, so we return -score\n",
    "    return -metrics['score']\n",
    "\n",
    "hpo_session_id = str(int(100*time.time()))\n",
    "\n",
    "def save_stats_hpo(display: bool):\n",
    "    res = stats_hpo_to_str()\n",
    "    try:\n",
    "        path = os.path.join(directory, 'hpo_'+hpo_session_id+\".txt\")\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(res)\n",
    "    except Exception as e:\n",
    "        print(f'failed to save hp')\n",
    "    if display:\n",
    "        print(res)\n",
    "\n",
    "# path to the last model saved\n",
    "last_path_for_save_model = None\n",
    "        \n",
    "def save_model(hyperparameters: dict, metrics: dict) -> None:\n",
    "    global last_path_for_save_model\n",
    "    try:\n",
    "        score = metrics['score']\n",
    "        path = os.path.join(directory, 'hpo_'+hpo_session_id+'_'+hyperparameters['caracteristiques_a_utiliser']+'_'+str(score)+\".txt\")\n",
    "        with open(path, 'w') as f:\n",
    "            text = hyperparameters_to_str(hyperparameters)\n",
    "            for m, v in metrics.items():\n",
    "                text += f'\\n#{m}={v}'\n",
    "            f.write(text)\n",
    "        try:\n",
    "            if last_path_for_save_model and os.path.isfile(last_path_for_save_model):\n",
    "                os.remove(last_path_for_save_model)\n",
    "        except Exception as e:\n",
    "            print(f'failed to delete file {last_path_for_save_model}: {e}')\n",
    "        last_path_for_save_model = path\n",
    "    except Exception as e:\n",
    "        print(f'failed to save hp: {e}')\n",
    "    \n",
    "def stats_hpo_to_str() -> str:\n",
    "    max_intervals = 10\n",
    "    result = \"\"\n",
    "    for hpo_key, hpo_key_stats in stats_hpo.items():\n",
    "        result += f\"stats for key '{hpo_key}':\\n\"\n",
    "        all_values = list(hpo_key_stats.keys())\n",
    "        old_value_to_new_value = dict()\n",
    "        if len(all_values)>max_intervals and (isinstance(all_values[0], int) or isinstance(all_values[0], float)):\n",
    "            min_value = float(min(all_values))\n",
    "            max_value = float(max(all_values))\n",
    "            for v in all_values:\n",
    "                index_interval = int (max_intervals * (float(v-min_value)/(max_value-min_value)))\n",
    "                index_interval = min(index_interval, max_intervals-1)\n",
    "                min_value_interval = min_value+index_interval* (max_value-min_value)/max_intervals\n",
    "                max_value_interval = min_value_interval+ (max_value-min_value)/max_intervals\n",
    "                new_key = f'[{round(min_value_interval,2)}, {round(max_value_interval,2)}]'\n",
    "                if new_key not in old_value_to_new_value:\n",
    "                    old_value_to_new_value[new_key] = [0,0]\n",
    "                old_value_to_new_value[new_key][0] += hpo_key_stats[v][0]\n",
    "                old_value_to_new_value[new_key][1] += hpo_key_stats[v][1]\n",
    "        else:\n",
    "            for v in all_values:\n",
    "                old_value_to_new_value[v] = hpo_key_stats[v]     \n",
    "        \n",
    "        for value, value_stats in sorted(old_value_to_new_value.items(), key=lambda item: item[1][1]/item[1][0], reverse=True):\n",
    "            count = value_stats[0]\n",
    "            avg_score = value_stats[1]/count\n",
    "            result += f'\\t{value} : {avg_score} ({count} samples)\\n'\n",
    "    return result\n",
    "        \n",
    "def extract_caracteristique_id(key: str):\n",
    "    if key == 'caracteristiques_a_utiliser' or key == 'label_si_resultats_differents':\n",
    "        return None\n",
    "    splitted = key.split('_')\n",
    "    if len(splitted)>=2 and len(splitted[-1]) >=1 and splitted[-1][0].isdigit():\n",
    "        return splitted[-1]\n",
    "    return None\n",
    "\n",
    "# When conducting an HPO search, some hyperparameters may exhibit inconsistent values.\n",
    "# This method aims to address those inconsistencies.\n",
    "def fix_hyperparameters(hyperparameters: dict) -> dict :\n",
    "    res = dict(hyperparameters)\n",
    "    for key in list(res.keys()):\n",
    "        caracteristique_id = extract_caracteristique_id(key)\n",
    "        if caracteristique_id and caracteristique_id not in res['caracteristiques_a_utiliser'].split('+'):\n",
    "            del res[key]\n",
    "    if '+' not in res['caracteristiques_a_utiliser']:\n",
    "        res['label_si_resultats_differents'] = None\n",
    "    return res\n",
    "\n",
    "\n",
    "# Transform the dictionary of hyperparameters 'hyperparameters' into string.\n",
    "def hyperparameters_to_str(hyperparameters: dict) -> str:\n",
    "    sorted_hyperparameters = sorted (hyperparameters.items())\n",
    "    return \"\\n\".join([hyperparameter_name+\" = \"+str(hyperparameter_value) for (hyperparameter_name,hyperparameter_value) in sorted_hyperparameters if hyperparameter_value is not None])\n",
    "\n",
    "def get_model_name(hyperparameters: dict) -> str:\n",
    "    file_content = hyperparameters_to_str(hyperparameters)\n",
    "    return compute_hash(file_content, 10)\n",
    "\n",
    "def compute_hash(input_string, max_length):\n",
    "    # Calculate MD5 hash of the input string\n",
    "    md5_hash = hashlib.md5(input_string.encode('ascii')).hexdigest().upper()\n",
    "    # Return the hash truncated to the max_length\n",
    "    return md5_hash[:max_length]\n",
    "\n",
    "def launch_hpo_for_transformer_model(max_evals: int):\n",
    "    seed = random.randint(0, 100000)\n",
    "    print(f'using seed: {seed}')\n",
    "    rstate = np.random.default_rng(seed)\n",
    "    best_indexes = fmin(\n",
    "        fn=objective,  # \"Loss\" function to minimize\n",
    "        space=search_space,  # Hyperparameter space\n",
    "        #algo=hyperopt_rand.suggest, #Random Search\n",
    "        algo=tpe.suggest,  # Tree-structured Parzen Estimator (TPE)\n",
    "        max_evals=max_evals,  # Perform 'max_evals' trials\n",
    "        max_queue_len = 10,\n",
    "        rstate =rstate,\n",
    "    )\n",
    "\n",
    "    # Get the best parameters\n",
    "    best  = space_eval(search_space, best_indexes)\n",
    "    print(f\"Found minimum after {max_evals} trials:\")\n",
    "    print([c for c in best.items() if c[1] is not None])\n",
    "    return best\n",
    "\n",
    "\n",
    "\n",
    "def float_range(x1: float, x2: float, count:int = 100) -> List[float]:\n",
    "    epsilon = (x2-x1)/count\n",
    "    # Generate the range of float values with the specified step\n",
    "    return list(np.arange(x1, x2, epsilon))\n",
    "\n",
    "def int_range(x:int, y:int, count:int = 100) -> List[int]:\n",
    "    epsilon = int ((y+1-x)/count )\n",
    "    epsilon = max(epsilon, 1)\n",
    "    return list(range(x, y+1,epsilon))\n",
    "\n",
    "\n",
    "\n",
    "search_space = {\n",
    "\n",
    "    # 1ere caracteristique version 1A : sujet sain si le pourcentage de points dans un intervalle donné est supérieur à un seuil (avec ajustement à la moyenne).\n",
    "    'min_value_1A': hp.choice('min_value_1A', int_range(80,120)), # 95 104 93\n",
    "    'range_1A': hp.choice('range_1A', int_range(50,150)), # 125 , 115 , 85  89\n",
    "    'seuil_1A': hp.choice('seuil_1A', float_range(0.9, 1.0)), # 0.95 0.96 0.97\n",
    "\n",
    "    # complément de la caractéristique 1A: sujet sain si le pourcentage de points dans un intervalle donné est inférieur à un seuil (avec ajustement à la moyenne).\n",
    "    'min_value_1Ai': hp.choice('min_value_1Ai', int_range(15,50)),  # 35 , 20, 22 20\n",
    "    'range_1Ai': hp.choice('range_1Ai', int_range(50,100)), # 65 ,64 87 72\n",
    "    'seuil_1Ai': hp.choice('seuil_1Ai', float_range(0.0, 0.1)), # 0.04 , 0.03 0.01  0.053 0.03\n",
    "\n",
    "    # 1ere caracteristique version 1B : sujet sain si le pourcentage de points dans un intervalle donné est supérieur à un seuil (sans ajustement à la moyenne).\n",
    "    'min_value_1B': hp.choice('min_value_1B', int_range(80,120)), # 93 97\n",
    "    'range_1B': hp.choice('range_1B', int_range(20,100)), # 72 61\n",
    "    'seuil_1B': hp.choice('seuil_1B', float_range(0.8, 1.0, 200)), # 0.906 0.882\n",
    "\n",
    "    # complément de la caractéristique 1B: sujet sain si le pourcentage de points dans un intervalle donné est inférieur à un seuil (sans ajustement à la moyenne).\n",
    "    'min_value_1Bi': hp.choice('min_value_1Bi', int_range(10,100)), # 14  60\n",
    "    'range_1Bi': hp.choice('range_1Bi', int_range(20,100)), # 70 30\n",
    "    'seuil_1Bi': hp.choice('seuil_1Bi', float_range(0.0, 0.1)), # 0.03 0.024\n",
    "\n",
    "    # 3eme caracteristique: sujet sain si l'étendue de l'électrocardiogramme est inférieur à un seuil.\n",
    "    'seuil_3': hp.choice('seuil_3', float_range(50,250, 1000)), # 124\n",
    "\n",
    "    # caracteristique 4A: sujet sain si le pourcentage de points autour de la moyenne est supérieur à un seuil.\n",
    "    'range_4A': hp.choice('range_4A', float_range(10,50)), # 43 44\n",
    "    'seuil_4A': hp.choice('seuil_4A', float_range(0.90, 0.99,1000)), # 0.96 0.97\n",
    "\n",
    "    # caracteristique 4B: sujet sain si le pourcentage de points autour de la médiane est supérieur à un seuil.\n",
    "    'range_4B': hp.choice('range_4B', float_range(10,50)), # 37  25 43\n",
    "    'seuil_4B': hp.choice('seuil_4B', float_range(0.90, 0.99,1000)), # 0.96\n",
    "\n",
    "    # caracteristique 5A: sujet sain si la moyenne de l'électrocardiogramme est dans l'intervalle [k1, k2]\n",
    "    #'min_value_5A': hp.choice('min_value_5A', list(range(122-40,122+40+1,1))),\n",
    "    #'seuil_5A': hp.choice('seuil_5A', list(range(9-6,9+6+1,1))),\n",
    "    'min_value_5A': hp.choice('min_value_5A', int_range(100,170)), # 124 142\n",
    "    'range_5A': hp.choice('range_5A', int_range(0,30)), # 11 14\n",
    "\n",
    "    # complément de la caractéristique 5A\n",
    "    'min_value_5Ai': hp.choice('min_value_5Ai', int_range(100,170)), # 131 133\n",
    "    'range_5Ai': hp.choice('range_5Ai', int_range(0,30)), #9 11\n",
    "\n",
    "    # caracteristique 5B: sujet sain si la moyenne de l'électrocardiogramme est dans l'intervalle [k1, k2]\n",
    "    'min_value_5B': hp.choice('min_value_5B', int_range(50,170)), # 110\n",
    "    'range_5B': hp.choice('range_5B', int_range(0,50)), # 28\n",
    "   \n",
    "    # caracteristique 6: sujet sain si le % de points au dessus de la moyenne de l'électrocardiogramme est inférieur à un seuil.\n",
    "    #'seuil_6': hp.choice('seuil_6', [(i * 0.01) for i in range(61-30, 61+30+1,1)]),\n",
    "    'seuil_6': hp.choice('seuil_6', float_range(0.0, 1.0, 1000)), # 0.629\n",
    "\n",
    "    # caracteristique 7A: sujet sain si la volatilité de l'électrocardiogramme est inférieur à un seuil.\n",
    "    'seuil_7A': hp.choice('seuil_7A', float_range(18.0, 22.0, 1000)), # 19.25 19.43\n",
    "    \n",
    "    # caracteristique 7B: sujet sain si l'écart type autour de la médiane de l'électrocardiogramme est inférieur à un seuil.\n",
    "    'seuil_7B': hp.choice('seuil_7B', float_range(18.0, 22.0, 1000)), # 19.636  19.615\n",
    "    \n",
    "    # 8eme caracteristique: sujet sain si le pourcentage de points dans l'intervalle [ k1 * moyenne, (1+k2) * moyenne] est supérieur à un seuil.\n",
    "    'k1_8': hp.choice('k1_8', float_range(0.5, 0.9)), # 0.66  0.72  0.71 \n",
    "    'k2_8': hp.choice('k2_8', float_range(0.08, 0.2)),  # 0.13 0.14 0.1\n",
    "    'seuil_8': hp.choice('seuil_8', float_range(0.8, 1.0)), #0.87 0.96 0.84\n",
    "\n",
    "    # caracteristique 9: sujet sain si le pourcentage de points dans l'intervalle [ 0, k] est inférieur à un seuil.\n",
    "    'k_9': hp.choice('k_9', float_range(50, 150,1000)), # 86\n",
    "    'seuil_9': hp.choice('seuil_9', float_range(0.0, 0.1)  ), # 0.03 0.017\n",
    "\n",
    "    # complément de la caractéristique 9\n",
    "    'k_9i': hp.choice('k_9i', float_range(150,200)), # 156  151\n",
    "    'seuil_9i': hp.choice('seuil_9i', float_range(0.8, 1.0)), # 0.94 0.885\n",
    "\n",
    "    # caracteristique 13: sujet sain si le pourcentage de fois où une valeur 'k' a été passée est > à un seuil.\n",
    "    'k_13': hp.choice('k_13', int_range(100, 180)),  # 139 140\n",
    "    'seuil_13': hp.choice('seuil_13', float_range(0.01, 0.03)), #0.0165  0.015\n",
    "\n",
    "    # complément de la caractéristique 13\n",
    "    'k_13i': hp.choice('k_13i', int_range(50, 90)),  #67 73 72\n",
    "    'seuil_13i': hp.choice('seuil_13i', float_range(0.0001, 0.00035)), # 0.00015 0.00020  0.00024\n",
    "\n",
    "    # caracteristique 14A: sujet sain si le pourcentage de passage par la moyenne est > à un seuil.\n",
    "    'seuil_14A': hp.choice('seuil_14A', float_range(0.01, 0.03, 1000)), #0.01989 0.017\n",
    "    \n",
    "    # caracteristique 14B: sujet sain si le pourcentage de passage par la médiane est > à un seuil.\n",
    "    'seuil_14B': hp.choice('seuil_14B', float_range(0.01, 0.03, 1000)), #0.0158\n",
    "    \n",
    "    'caracteristiques_a_utiliser': hp.choice('caracteristiques_a_utiliser', ['14A']),\n",
    "    'label_si_resultats_differents': hp.choice('label_si_resultats_differents', [0, 1]),\n",
    "}\n",
    "\n",
    "\n",
    "if len(sys.argv) >=2 and str.isdigit(sys.argv[1][0]):\n",
    "    caracteristiques_a_utiliser = sys.argv[1].split(',')\n",
    "    print(f'caracteristiques_a_utiliser will be set to {caracteristiques_a_utiliser}')\n",
    "    search_space['caracteristiques_a_utiliser'] =  hp.choice('caracteristiques_a_utiliser', caracteristiques_a_utiliser)\n",
    "\n",
    "\n",
    "max_evals = 10\n",
    "if len(sys.argv) >=3 and str.isdigit(sys.argv[2]):\n",
    "    print(f'max_evals will be set to {sys.argv[2]}')\n",
    "    max_evals = int(sys.argv[2])\n",
    "print(f'max_evals value is {max_evals}')\n",
    "\n",
    "# Uncomment following line to enable HPO\n",
    "already_processed_model_names_with_hpo = dict()\n",
    "start_time = time.time()\n",
    "best = launch_hpo_for_transformer_model(max_evals)\n",
    "print(f'hpo took {round(time.time()-start_time,2)}s')\n",
    "save_stats_hpo(True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
