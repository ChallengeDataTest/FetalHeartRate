{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fab4cd",
   "metadata": {},
   "source": [
    "# Le but de ce notebook est d'identifier des caractéristiques très simples permettant d'analyser des battements de coeur de foetus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e01dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install hyperopt\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# permet de charger les fichiers matlab (*.mat)\n",
    "from scipy.io import loadmat\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from typing import List,Set,Tuple,Dict\n",
    "\n",
    "# le répertoire de travail\n",
    "directory = os.path.abspath('')\n",
    "\n",
    "# répertoire où se trouvent toutes les données associés à ce challenge\n",
    "data_directory = os.path.join(directory, 'Data')\n",
    "\n",
    "# fichier CSV contenant les targets (1 / 0)\n",
    "targets_path = os.path.join(data_directory, 'CTG_Challenge_files_GroundTruth.csv')\n",
    "\n",
    "# répertoire où se trouvent les fichiers de données matlab\n",
    "matlab_directory = os.path.join(data_directory, 'ctg_workshop_database')\n",
    "\n",
    "# dans l'électrocardiogramme, nous avons 4 mesures par seconde\n",
    "# chaque mesure correspond au nombre de battements de coeurs par minute\n",
    "elements_en_1s = 4\n",
    "elements_en_1minute = int(elements_en_1s*60)\n",
    "elements_en_5minutes = 5*elements_en_1minute\n",
    "elements_en_30minutes = 30*elements_en_1minute\n",
    "elements_en_1heure = 60*elements_en_1minute\n",
    "\n",
    "# retourne tous les fichiers matlab présents dans le repertoire 'path'\n",
    "def all_mat_files_in_directory(path: str):\n",
    "    return [os.path.join(path,f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith('.mat')]\n",
    "\n",
    "# calcule la moyenne de la séquence ''fhr' en ignorant les NaN\n",
    "def moyenne(fhr):\n",
    "    return fhr[~np.isnan(fhr)].mean()\n",
    "\n",
    "# calcule la médiane de la séquence ''fhr' en ignorant les NaN\n",
    "def mediane(fhr):\n",
    "    return np.nanmedian(fhr)\n",
    "\n",
    "# calcule la std dev de la séquence ''fhr' en ignorant les NaN\n",
    "def ecart_type(fhr):\n",
    "    return fhr[~np.isnan(fhr)].std()\n",
    "\n",
    "def proportion_in_interval(fhr, min_value: float, max_value: float) -> float:\n",
    "    total_points = np.count_nonzero(~np.isnan(fhr))\n",
    "    if total_points<=0:\n",
    "        return 0\n",
    "    total_points_in_interval = np.count_nonzero((fhr > min_value) & (fhr < max_value) )\n",
    "    return total_points_in_interval/total_points\n",
    "\n",
    "\n",
    "# calcul des 3 quartiles Q1 , Q2 (=mediane), Q3 (en ignorant les NaN)\n",
    "def compute_quartiles_Q1_Q2_Q3(fhr) -> Tuple[float,float,float]:\n",
    "    # Remove NaN values\n",
    "    cleaned_data = fhr[~np.isnan(fhr)]\n",
    "    Q1 = np.percentile(cleaned_data, 25)\n",
    "    Q2 = np.percentile(cleaned_data, 50)  # This is the median\n",
    "    Q3 = np.percentile(cleaned_data, 75)\n",
    "    return Q1,Q2,Q3\n",
    "\n",
    "def create_dataset_stats(dataset: dict) -> str: \n",
    "    count_target_0 = 0\n",
    "    count_target_1 = 0\n",
    "    for id in id_to_fhr_last_hour.keys():\n",
    "        if id_to_target[id] == 0:\n",
    "            count_target_0 += 1\n",
    "        else:\n",
    "            count_target_1 += 1\n",
    "    count_total = count_target_0+count_target_1\n",
    "    return f'{(count_total)} elements in dataset: {count_target_0} for target 0 ({round(100*count_target_0/count_total,1)}%) and {count_target_1} for target 1 ({round(100*count_target_1/count_total,1)}%)'\n",
    "\n",
    "# calcul de l'étendue\n",
    "def compute_range(fhr) -> float:\n",
    "    return np.nanmax(fhr)-np.nanmin(fhr)\n",
    "\n",
    "# nombre d elements NaN dans la séquence 'fhr'\n",
    "def nan_count(fhr):\n",
    "    return np.count_nonzero(np.isnan(fhr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7142cf8",
   "metadata": {},
   "source": [
    "# PRIVE: Chargement des données d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d009cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chemin vers tous les fichiers matlab de la base d'entraînement\n",
    "id_to_path = dict()\n",
    "for filename in all_mat_files_in_directory(matlab_directory):\n",
    "    filename_without_extension = os.path.splitext(os.path.basename(filename))[0]    \n",
    "    id = int(filename_without_extension.lstrip('0'))\n",
    "    id_to_path[id] = filename\n",
    "\n",
    "# on charge les targets associés à chaque fichier d'entraînement\n",
    "targets_df = pd.read_csv(targets_path)\n",
    "id_to_target = dict()\n",
    "for _, row in targets_df.iterrows():\n",
    "    id_to_target[row['ChallengeID']] = row['TrueOutcome']\n",
    "\n",
    "# lecture des battements de coeurs des foetus\n",
    "id_to_fhr_full = dict()\n",
    "id_to_fhr_last_hour = dict() #uniquement la dernière heure\n",
    "all_lengths = []\n",
    "for id, path in id_to_path.items():\n",
    "    matlab_file = loadmat(path)\n",
    "    id_to_fhr_full[id] = matlab_file['fhr'].flatten()\n",
    "    id_to_fhr_last_hour[id] = copy.copy(id_to_fhr_full[id][-60*4*60:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d15ddac",
   "metadata": {},
   "source": [
    "# PRIVE: Calcul de statistiques sur ces données d'entraînement\n",
    "## (pour faciliter la recherche des caractéristiques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "targets = []\n",
    "mean_last_hour = []\n",
    "std_dev_last_hour = []\n",
    "count_last_hour = []\n",
    "nan_count_last_hour = []\n",
    "Q1_last_hour = []\n",
    "Q2_last_hour = []\n",
    "Q3_last_hour = []\n",
    "interquartile_range_last_hour = []\n",
    "etendue_last_hour = []\n",
    "percent_around_mean_last_hour = []\n",
    "percent_around_median_last_hour = []\n",
    "\n",
    "for id in list(id_to_fhr_last_hour.keys()):\n",
    "    ids.append(id)\n",
    "    targets.append(id_to_target[id])\n",
    "    fhr_last_hour = id_to_fhr_last_hour[id]\n",
    "    mean_last_hour.append(moyenne(fhr_last_hour))\n",
    "    std_dev_last_hour.append(ecart_type(fhr_last_hour)) \n",
    "    count_last_hour.append(fhr_last_hour.size)\n",
    "    nan_count_last_hour.append(nan_count(fhr_last_hour))\n",
    "    (Q1,Q2,Q3) = compute_quartiles_Q1_Q2_Q3(fhr_last_hour)\n",
    "    Q1_last_hour.append(Q1)\n",
    "    Q2_last_hour.append(Q2)\n",
    "    Q3_last_hour.append(Q3)\n",
    "    interquartile_range_last_hour.append(Q3-Q1)\n",
    "    etendue_last_hour.append(compute_range(fhr_last_hour)) \n",
    "    mean = moyenne(fhr_last_hour)\n",
    "    percent_around_mean_last_hour.append(proportion_in_interval(fhr_last_hour, mean-32.8, mean+32.8))\n",
    "    median = mediane(fhr_last_hour)\n",
    "    percent_around_median_last_hour.append(proportion_in_interval(fhr_last_hour, median-31.8, median+31.8))\n",
    "\n",
    "    \n",
    "# Sauvegarde de ces statistiques dans un DataFrame\n",
    "fhr_stats = pd.DataFrame(\n",
    "    {'ids': ids,\n",
    "    'targets': targets,\n",
    "    'mean_last_hour' : mean_last_hour,\n",
    "    'std_dev_last_hour' : std_dev_last_hour,\n",
    "    'count_last_hour' : count_last_hour,\n",
    "    'nan_count_last_hour' : nan_count_last_hour,\n",
    "    'Q1_last_hour' : Q1_last_hour,\n",
    "    'Q2_last_hour' : Q2_last_hour,\n",
    "    'Q3_last_hour' : Q3_last_hour,\n",
    "    'interquartile_range_last_hour' : interquartile_range_last_hour,\n",
    "    'etendue_last_hour' : etendue_last_hour,\n",
    "    'percent_around_mean_last_hour' : percent_around_mean_last_hour,\n",
    "    'percent_around_median_last_hour' : percent_around_median_last_hour,\n",
    "    })\n",
    "\n",
    "# on sauvegarde ces stats sur le disque\n",
    "fhr_stats.to_csv(os.path.join(directory, 'fhr_stats.csv'), index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38372f49",
   "metadata": {},
   "source": [
    "# PRIVE: On supprime les données qui semblent mal labelisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dde389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of controversial elements to discard    \n",
    "# for instance:\n",
    "#   0.1 means that the 10% of most controversial elements will be discarded\n",
    "#   0.0 means that no controverisal elements will be discarded\n",
    "\n",
    "percentage_controversial_to_discard = 1/6\n",
    "\n",
    "print(f'Initial dataset before removing any controversials elements:')\n",
    "print(f'\\t{create_dataset_stats(id_to_fhr_last_hour)}')\n",
    "\n",
    "\n",
    "controversials = [27,81,267,274,192,26,153,126,8,161,298,270,67,167,188,210,185,214,172,135,109,196,79,105,297,104,69,21,64,233,90,211,175,248,78,98,180,139,205,217,22,272,12,250,46,283,103,57,150,91,289,164,290,5,291,110,59,295,68,224,232,284,70,87,124,244,17,71,123,241,213,39,49,268,173,234,2,166,56,294,247,170,41,33,243,181,171,235,102,145,108,97,92,34,133,147,251,230,117,249,252,197,203,43,269,25,245,121,281,207,138,242,220,201,20,144,143,101,44,106,263,278,28,200,114,184,76,218,16,276,149,53,14,47,141,125,48,221,193,165,115,52,31,191,199,253,37,186,187,51,288,163,179,38,282,183,219,287,177,174,18,254,42,99,95,258,122,132,209,24,130,169,32,96,131,4,212,231,190,29,93,137,157,120,72,206,62,286,127,277,74,256,225,146,222,86,178,236,19,158,65,259,89,208,168,77,140,257,154,50,7,40,118,58,264,299,54,237,204,36,195,61,94,84,119,266,260,1,85,189,226,45,128,134,142,113,9,83,116,279,35,162,159,112,275,88,55,156,223,100,296,280,80,246,215,271,229,107,160,152,202,111,63,82,30,60,194,273,300,75,176,15,292,265,227,293,262,148,151,136,239,73,182,66,129,238,3,261,23,6,216,13,155,228,255,198,285,10,11,240]\n",
    "\n",
    "# to_remove_count[0] : number of elements to remove with target == 0\n",
    "# to_remove_count[1] : number of elements to remove with target == 1\n",
    "to_remove_count = [int(percentage_controversial_to_discard*list(id_to_target.values()).count(0)), int(percentage_controversial_to_discard*list(id_to_target.values()).count(1))]\n",
    "\n",
    "#we discard controversial data (with possible invalid label)\n",
    "for id in controversials:\n",
    "    if id in id_to_fhr_last_hour and to_remove_count[id_to_target[id]] > 0 :\n",
    "        to_remove_count[id_to_target[id]] -= 1\n",
    "        del id_to_path[id]\n",
    "        del id_to_target[id]\n",
    "        del id_to_fhr_full[id]\n",
    "        del id_to_fhr_last_hour[id]\n",
    "\n",
    "print(f'Dataset after removing {round(100*percentage_controversial_to_discard,1)}% of most controversials elements:')\n",
    "print(f'\\t{create_dataset_stats(id_to_fhr_last_hour)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1bd514",
   "metadata": {},
   "source": [
    "# PRIVE: méthodes permettant l'affichage d'électrocardiogrammes et d'histogrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d1cbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def interpolate_nans(array):\n",
    "    not_nan = ~np.isnan(array)\n",
    "    indices = np.arange(len(array))\n",
    "    interpolated_array = np.copy(array)\n",
    "    interpolated_array[np.isnan(array)] = np.interp(indices[np.isnan(array)], indices[not_nan], array[not_nan])\n",
    "    return interpolated_array\n",
    "\n",
    "def display_electrocardiogram(id: int, start_minut:float, duration_in_minuts: float, display_mean: bool = True, interpolate_missing_values:bool = True, min_y_value: int = None, max_y_value: int = None):\n",
    "    fhr_full = loadmat(id_to_path[id])['fhr'].ravel()\n",
    "    start_idx = int(start_minut*4*60)\n",
    "    if start_idx<0: \n",
    "        start_idx+=len(fhr_full)\n",
    "    start_idx = max(0, start_idx-1)\n",
    "    end_idx = min( start_idx+1+int(duration_in_minuts*4*60) , len(fhr_full) )\n",
    "    \n",
    "    fhr = fhr_full[start_idx:end_idx]\n",
    "    if interpolate_missing_values:\n",
    "        fhr = interpolate_nans(fhr)\n",
    "    \n",
    "    # nombre d'éléments dans l'électrocardiogramme\n",
    "    n = len(fhr)\n",
    "    \n",
    "    # Create an array of time points (assuming each heart rate measurement is taken at regular intervals)\n",
    "    time_in_minuts = np.arange(n)/(60*4)\n",
    "    # Plot the heart rate data\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.plot(time_in_minuts, fhr, linestyle='-', color='black', linewidth=1)\n",
    "    # Formater les ticks pour afficher le temps au format hh:mm\n",
    "    def format_func(time_in_minuts, tick_number):\n",
    "        #time_in_minuts = int(time_in_minuts/(60*4))\n",
    "        hours = int(time_in_minuts) // 60\n",
    "        minutes = int(time_in_minuts) % 60\n",
    "        return f'{hours:02d}:{minutes:02d}'\n",
    "    plt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(format_func))\n",
    "    comment = 'sujet sain' if id_to_target[id] == 0 else 'sujet malade'\n",
    "    \n",
    "    if display_mean:\n",
    "        observed_mean = int(moyenne(fhr))\n",
    "        plt.axhline(y=observed_mean, color='red', linewidth=1, linestyle='-', label='Moyenne: '+str(observed_mean))\n",
    "        # Annotate the y-value on the vertical axis\n",
    "        plt.text(x=0, y=observed_mean, s=f'{observed_mean:.0f}', color='red', va='center', ha='right')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Temps (minutes)', fontsize=15)\n",
    "    plt.ylabel('Battements de coeur par minutes', fontsize=15)\n",
    "    plt.title(f\"Electrocardiogramme pour un {comment} ({id})\", fontsize=15)\n",
    "    plt.xlim(0, max(time_in_minuts))\n",
    "    plt.ylim(min_y_value or 60, max_y_value or 180)\n",
    "    # Display grid\n",
    "    plt.grid(True)\n",
    "\n",
    "    if display_mean:\n",
    "        plt.legend()\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def display_histogram(id: int, start_minut:float, duration_in_minuts: float, display_mean: bool = True):\n",
    "    fhr_full = loadmat(id_to_path[id])['fhr'].ravel()\n",
    "    start_idx = int(start_minut*4*60)\n",
    "    if start_idx<0: \n",
    "        start_idx+=len(fhr_full)\n",
    "    start_idx = max(0, start_idx-1)\n",
    "    end_idx = min( start_idx+1+int(duration_in_minuts*4*60) , len(fhr_full) )\n",
    "    data = fhr_full[start_idx:end_idx]\n",
    "    # we discard NaN\n",
    "    data = data[~np.isnan(data)] \n",
    "    plt.figure(figsize=(16, 6))  # Increase the width for better horizontal display\n",
    "    # Plotting the histogram\n",
    "    counts, bins, patches = plt.hist(data, bins=30, edgecolor='black', weights=[100/len(data)]*len(data))\n",
    "    \n",
    "    # Adding title and labels\n",
    "    comment = 'sujet sain' if id_to_target[id] == 0 else 'sujet malade'\n",
    "\n",
    "    plt.title(\"Histogramme\")\n",
    "    plt.xlabel('Nombre de battements de coeur à la minute')\n",
    "    plt.ylabel('Fréquence (%)')\n",
    "\n",
    "    if display_mean:\n",
    "        observed_mean = int(moyenne(data))\n",
    "        plt.axvline(observed_mean, color='red', linestyle='-', linewidth=2)\n",
    "        plt.text(observed_mean, plt.ylim()[1]*0.9, f'Moyenne: {observed_mean:.2f}', color='red', ha='center')\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def display_electrocardiograms(count:int, start_minut:int, duration_in_minuts: int, label: int, start_id:int = 1, display_mean: bool = True, interpolate_missing_values:bool = True, min_y_value: int = None, max_y_value: int = None):\n",
    "    displayed_count = 0\n",
    "    for idx in range(start_id, start_id+300,1):\n",
    "        id = idx if idx <=300 else idx-300\n",
    "        if id not in id_to_target:\n",
    "            continue\n",
    "        if id_to_target[id] != label:\n",
    "            continue\n",
    "        display_electrocardiogram(id, start_minut, duration_in_minuts, display_mean=display_mean, interpolate_missing_values=interpolate_missing_values, min_y_value=min_y_value, max_y_value=max_y_value)\n",
    "        display_histogram(id, start_minut, duration_in_minuts, display_mean=display_mean)\n",
    "        displayed_count+= 1\n",
    "        if displayed_count>=count:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5bab3a",
   "metadata": {},
   "source": [
    "# Affichage d'exemples de données pour des sujets sains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91be5329",
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_a_afficher = 10\n",
    "start_minut = -60\n",
    "duration_in_minuts = 30\n",
    "label = 0  #0 pour les sujets sains , 1 pour les sujets malades\n",
    "start_id =1 # id du 1er élément à afficher\n",
    "\n",
    "display_electrocardiograms(nombre_a_afficher, start_minut, duration_in_minuts, label, start_id, display_mean=True, interpolate_missing_values=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec7c4c",
   "metadata": {},
   "source": [
    "# Affichage d'exemples de données pour des sujets malades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad19cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_a_afficher = 10\n",
    "start_minut = -60\n",
    "duration_in_minuts = 30\n",
    "label = 1  #0 pour les sujets sains , 1 pour les sujets malades\n",
    "start_id = 1 # id du 1er élément à afficher\n",
    "\n",
    "\n",
    "display_electrocardiograms(nombre_a_afficher, start_minut, duration_in_minuts, label, start_id, display_mean=True, interpolate_missing_values=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495f78b8",
   "metadata": {},
   "source": [
    "# PRIVE: Liste des caractéristiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505aa43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def proportion_in_interval_last_hour(id: int, min_value: float, max_value: float) -> float:\n",
    "    return proportion_in_interval(id_to_fhr_last_hour[id], min_value, max_value)\n",
    "\n",
    "# calcul de l'écart type\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def compute_id_std_dev_last_hour(id: int):\n",
    "    return ecart_type(id_to_fhr_last_hour[id])\n",
    "\n",
    "# caracteristique 1: sujet sain si l'écart-type est inférieur à un seuil.\n",
    "def calcul_caracteristique1(id: int, hyperparameters: dict, suffix: str) -> int:\n",
    "    return 0 if compute_id_std_dev_last_hour(id) < hyperparameters['seuil_'+suffix] else 1\n",
    "\n",
    "# calcul de l'écart interquartile (en ignorant les NaN)\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def compute_id_interquartile_range_last_hour(id: int) -> float:\n",
    "    (Q1,Q2,Q3) = compute_quartiles_Q1_Q2_Q3(id_to_fhr_last_hour[id])\n",
    "    return Q3-Q1\n",
    "\n",
    "# caracteristique 2: sujet sain si l'écart interquartile est inférieur à un seuil\n",
    "def calcul_caracteristique2(id: int, hyperparameters: dict, suffix: str) -> int:\n",
    "    return 0 if compute_id_interquartile_range_last_hour(id) < hyperparameters['seuil_'+suffix] else 1\n",
    "\n",
    "# calcul de l'etendue de la séquence associée à l'id 'id' en ignorant les NaN\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def compute_id_range_last_hour(id: int):\n",
    "    return compute_range(id_to_fhr_last_hour[id])\n",
    "\n",
    "# caracteristique 3: sujet sain si l'étendue est inférieure à un seuil.\n",
    "def calcul_caracteristique3(id: int, hyperparameters: dict, suffix: str) -> int:\n",
    "    return 0 if compute_id_range_last_hour(id)< hyperparameters['seuil_'+suffix] else 1\n",
    "\n",
    "\n",
    "# calcul de la moyenne de la séquence associée à l'id 'id' en ignorant les NaN\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def compute_id_mean_last_hour(id: int):\n",
    "    return moyenne(id_to_fhr_last_hour[id])\n",
    "\n",
    "# 4eme caracteristique: sujet sain si le pourcentage de points autour de la moyenne est supérieur à un seuil.\n",
    "def calcul_caracteristique4(id: int, hyperparameters: dict, suffix: str) -> int:\n",
    "    range = hyperparameters['range_'+suffix]\n",
    "    moyenne = compute_id_mean_last_hour(id)\n",
    "    return 0 if proportion_in_interval_last_hour(id, moyenne-range, moyenne+range) > hyperparameters['seuil_'+suffix] else 1\n",
    "\n",
    "\n",
    "\n",
    "# calcul de la médiane de la séquence associée à l'id 'id' en ignorant les NaN\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def compute_id_median_last_hour(id: int):\n",
    "    return mediane(id_to_fhr_last_hour[id])\n",
    "\n",
    "# 5eme caracteristique: sujet sain si le pourcentage de points autour de la médiane est supérieur à un seuil.\n",
    "def calcul_caracteristique5(id: int, hyperparameters: dict, suffix: str) -> int:\n",
    "    range = hyperparameters['range_'+suffix]\n",
    "    median = compute_id_mean_last_hour(id)\n",
    "    return 0 if proportion_in_interval_last_hour(id, median-range, median+range) > hyperparameters['seuil_'+suffix] else 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6398b39",
   "metadata": {},
   "source": [
    "# PRIVE: Hyperparameters Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04040a0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import math\n",
    "from hyperopt import fmin, tpe, space_eval, hp, Trials, rand as hyperopt_rand\n",
    "import hashlib\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "\n",
    "current_lowest_error = None\n",
    "stats_hpo = dict()\n",
    "last_time_display_stats_hpo = time.time()\n",
    "\n",
    "def compute_error(TP: int, TN: int, FP: int, FN: int):\n",
    "    return 1.0-(TP+TN)/(TP+TN+FP+FN)\n",
    "    # to compute error as : (error(target0)+error(target1)/2 , uncomment following line:\n",
    "    #return (compute_error_target0(TP,TN,FP,FN)+compute_error_target1(TP,TN,FP,FN))/2\n",
    "    \n",
    "    \n",
    "def compute_f1_score(TP: int, TN: int, FP: int, FN: int):\n",
    "    return (2*TP)/(2*TP+FP+FN)\n",
    "        \n",
    "def compute_error_target0(TP: int, TN: int, FP: int, FN: int):\n",
    "    return 1.0-TN/(1*TN+FP)\n",
    "\n",
    "def compute_error_target1(TP: int, TN: int, FP: int, FN: int):\n",
    "    return 1.0-TP/(1*TP+FN)\n",
    "        \n",
    "def compute_matrice_de_confusion(id_to_predictions, id_to_target) -> (int, int, int,int):\n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        for id, data in id_to_predictions.items():\n",
    "            target = id_to_target[id]\n",
    "            prediction = id_to_predictions[id]\n",
    "            if prediction == target: # bonne prediction\n",
    "                if target == 1:\n",
    "                    TP += 1\n",
    "                else:\n",
    "                    TN += 1\n",
    "            else: # erreur dans la prediciton\n",
    "                if prediction == 1:\n",
    "                    FP += 1\n",
    "                else:\n",
    "                    FN += 1\n",
    "        return (TP,TN,FP,FN)\n",
    "\n",
    "def compute_single_prediction_single_caracteristique(id: str, hyperparameters: dict, caracteristique: str) -> int:  \n",
    "    return globals()['calcul_caracteristique'+caracteristique](id, hyperparameters, caracteristique)\n",
    "\n",
    "def compute_single_prediction(id: str, hyperparameters: dict) -> int:  \n",
    "    valeurs_caracteristiques_a_utiliser = []\n",
    "    for c in hyperparameters['caracteristiques_a_utiliser'].split('+'):\n",
    "        valeurs_caracteristiques_a_utiliser.append(compute_single_prediction_single_caracteristique(id, hyperparameters, c))\n",
    "    if not all_elements_same(valeurs_caracteristiques_a_utiliser):\n",
    "        return hyperparameters['label_si_resultats_differents']\n",
    "    return valeurs_caracteristiques_a_utiliser[0]\n",
    "    \n",
    "def all_elements_same(data):\n",
    "    first_element = data[0]\n",
    "    return all(element == first_element for element in data)\n",
    "\n",
    "def compute_all_predictions(hyperparameters: dict) -> dict:\n",
    "    id_to_predictions = dict()\n",
    "    for id in id_to_fhr_last_hour.keys():\n",
    "        id_to_predictions[id] = compute_single_prediction(id, hyperparameters)\n",
    "    return id_to_predictions\n",
    "    \n",
    "def train(hyperparameters: dict, verbose: bool) -> dict:\n",
    "    id_to_predictions = compute_all_predictions(hyperparameters)\n",
    "    (TP,TN,FP,FN) = compute_matrice_de_confusion(id_to_predictions, id_to_target)\n",
    "    metrics = dict()\n",
    "    metrics['TP'] = TP\n",
    "    metrics['TN'] = TN\n",
    "    metrics['FP'] = FP\n",
    "    metrics['FN'] = FN\n",
    "    metrics['error_target0'] = compute_error_target0(TP,TN,FP,FN)\n",
    "    metrics['error_target1'] = compute_error_target1(TP,TN,FP,FN)\n",
    "    current_error = compute_error(TP,TN,FP,FN)\n",
    "    metrics['erreur'] = current_error\n",
    "    global last_time_display_stats_hpo\n",
    "    if (time.time()-last_time_display_stats_hpo)>600:\n",
    "        last_time_display_stats_hpo = time.time()\n",
    "    global current_lowest_error\n",
    "    if not current_lowest_error or current_error<current_lowest_error:\n",
    "        current_lowest_error = current_error\n",
    "        if verbose:\n",
    "            print(f\"new lowest error {round(current_error,4)} for hyperparameters {[c for c in hyperparameters.items() if c[1] is not None]}\")\n",
    "    return metrics\n",
    "    \n",
    "# the objective used for Hyperparameters Optimization (HPO)\n",
    "# it is the function to minimize\n",
    "def objective(sample_from_search_space):\n",
    "    hyperparameters = fix_hyperparameters(sample_from_search_space)\n",
    "    model_name = get_model_name(hyperparameters)\n",
    "    if model_name in already_processed_model_names_with_hpo:\n",
    "        metrics = already_processed_model_names_with_hpo[model_name]\n",
    "    else:\n",
    "        metrics = train(hyperparameters, True)\n",
    "        already_processed_model_names_with_hpo[model_name] = metrics\n",
    "    # we want to minimize this mettric ('erreur')\n",
    "    return metrics['erreur']\n",
    "\n",
    "hpo_session_id = str(int(100*time.time()))\n",
    "\n",
    "def extract_caracteristique_id(key: str):\n",
    "    if key == 'caracteristiques_a_utiliser' or key == 'label_si_resultats_differents':\n",
    "        return None\n",
    "    splitted = key.split('_')\n",
    "    if len(splitted)>=2 and len(splitted[-1]) >=1 and splitted[-1][0].isdigit():\n",
    "        return splitted[-1]\n",
    "    return None\n",
    "\n",
    "# When conducting an HPO search, some hyperparameters may exhibit inconsistent values.\n",
    "# This method aims to address those inconsistencies.\n",
    "def fix_hyperparameters(hyperparameters: dict) -> dict :\n",
    "    res = dict(hyperparameters)\n",
    "    for key in list(res.keys()):\n",
    "        caracteristique_id = extract_caracteristique_id(key)\n",
    "        if caracteristique_id and caracteristique_id not in res['caracteristiques_a_utiliser'].split('+'):\n",
    "            del res[key]\n",
    "    if '+' not in res['caracteristiques_a_utiliser']:\n",
    "        res['label_si_resultats_differents'] = None\n",
    "    return res\n",
    "\n",
    "# Transform the dictionary of hyperparameters 'hyperparameters' into string.\n",
    "def hyperparameters_to_str(hyperparameters: dict) -> str:\n",
    "    sorted_hyperparameters = sorted (hyperparameters.items())\n",
    "    return \"\\n\".join([hyperparameter_name+\" = \"+str(hyperparameter_value) for (hyperparameter_name,hyperparameter_value) in sorted_hyperparameters if hyperparameter_value is not None])\n",
    "\n",
    "def get_model_name(hyperparameters: dict) -> str:\n",
    "    file_content = hyperparameters_to_str(hyperparameters)\n",
    "    return compute_hash(file_content, 10)\n",
    "\n",
    "def compute_hash(input_string, max_length):\n",
    "    # Calculate MD5 hash of the input string\n",
    "    md5_hash = hashlib.md5(input_string.encode('ascii')).hexdigest().upper()\n",
    "    # Return the hash truncated to the max_length\n",
    "    return md5_hash[:max_length]\n",
    "\n",
    "def launch_hpo(max_evals: int):\n",
    "    seed = random.randint(0, 100000)\n",
    "    print(f'using seed: {seed}')\n",
    "    rstate = np.random.default_rng(seed)\n",
    "    best_indexes = fmin(\n",
    "        fn=objective,  # \"Loss\" function to minimize\n",
    "        space=search_space,  # Hyperparameter space\n",
    "        algo=hyperopt_rand.suggest, #Random Search\n",
    "        #algo=tpe.suggest,  # Tree-structured Parzen Estimator (TPE)\n",
    "        max_evals=max_evals,  # Perform 'max_evals' trials\n",
    "        max_queue_len = 10,\n",
    "        rstate =rstate,\n",
    "    )\n",
    "\n",
    "    # Get the best parameters\n",
    "    best  = space_eval(search_space, best_indexes)\n",
    "    print(f\"Found minimum after {max_evals} trials:\")\n",
    "    print([c for c in best.items() if c[1] is not None])\n",
    "    return best\n",
    "\n",
    "\n",
    "\n",
    "def float_range(x1: float, x2: float, count:int = 100) -> List[float]:\n",
    "    epsilon = (x2-x1)/count\n",
    "    # Generate the range of float values with the specified step\n",
    "    return list(np.arange(x1, x2, epsilon))\n",
    "\n",
    "def int_range(x:int, y:int, count:int = 100) -> List[int]:\n",
    "    epsilon = int ((y+1-x)/count )\n",
    "    epsilon = max(epsilon, 1)\n",
    "    return list(range(x, y+1,epsilon))\n",
    "\n",
    "\n",
    "\n",
    "search_space = {\n",
    "\n",
    "    # caracteristique 1: sujet sain si l'écart type est inférieur à un seuil.\n",
    "    'seuil_1': hp.choice('seuil_1', float_range(15.0, 30.0, 1000)),\n",
    "    \n",
    "    # caracteristique 2: sujet sain si l'écart interquartile est inférieur à un seuil\n",
    "    'seuil_2': hp.choice('seuil_2', float_range(15, 40.0, 1000)),\n",
    "\n",
    "    # caracteristique 3: sujet sain si l'étendue est inférieure à un seuil.\n",
    "    'seuil_3': hp.choice('seuil_3', float_range(150,200, 100)),\n",
    "    \n",
    "    # caracteristique 4: sujet sain si le pourcentage de points autour de la moyenne est supérieur à un seuil.\n",
    "    'range_4': hp.choice('range_4', float_range(10,50)),\n",
    "    'seuil_4': hp.choice('seuil_4', float_range(0.80, 0.99,100)),\n",
    "\n",
    "    # caracteristique 5: sujet sain si le pourcentage de points autour de la médiane est supérieur à un seuil.\n",
    "    'range_5': hp.choice('range_5', float_range(30,35)),\n",
    "    'seuil_5': hp.choice('seuil_5', float_range(0.80, 0.90)),\n",
    "\n",
    "    'caracteristiques_a_utiliser': hp.choice('caracteristiques_a_utiliser', ['3']),\n",
    "    'label_si_resultats_differents': hp.choice('label_si_resultats_differents', [0, 1]),\n",
    "}\n",
    "\n",
    "\n",
    "if len(sys.argv) >=2 and str.isdigit(sys.argv[1][0]):\n",
    "    caracteristiques_a_utiliser = sys.argv[1].split(',')\n",
    "    print(f'caracteristiques_a_utiliser will be set to {caracteristiques_a_utiliser}')\n",
    "    search_space['caracteristiques_a_utiliser'] =  hp.choice('caracteristiques_a_utiliser', caracteristiques_a_utiliser)\n",
    "\n",
    "\n",
    "max_evals = 200\n",
    "if len(sys.argv) >=3 and str.isdigit(sys.argv[2]):\n",
    "    print(f'max_evals will be set to {sys.argv[2]}')\n",
    "    max_evals = int(sys.argv[2])\n",
    "print(f'max_evals value is {max_evals}')\n",
    "\n",
    "already_processed_model_names_with_hpo = dict()\n",
    "start_time = time.time()\n",
    "# Uncomment following line to enable HPO\n",
    "#best = launch_hpo(max_evals)\n",
    "print(f'hpo took {round(time.time()-start_time,2)}s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c810a07e",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border-width:0; color:black; background-color:black\">\n",
    "<span style=\"font-size: 48px;\">1ère partie</span>\n",
    "<hr style=\"height:2px; border-width:0; color:black; background-color:black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e92525",
   "metadata": {},
   "source": [
    "## Faire constater par l'étudiant que l'étendue est plus importante pour les sujets malades que pour les sujets sains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f843946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_stats_etendue():\n",
    "    etendue_target_0 =[]\n",
    "    etendue_target_1 =[]\n",
    "    for id in id_to_fhr_last_hour.keys():\n",
    "        etendue = compute_id_range_last_hour(id)\n",
    "        if 0 == id_to_target[id]:\n",
    "            etendue_target_0.append(etendue)\n",
    "        else:\n",
    "            etendue_target_1.append(etendue)\n",
    "    print(f\"Valeur de l'étendue:\")\n",
    "    print(f'\\tMoyenne pour les sujets sains = {round(np.mean(etendue_target_0),1)}')\n",
    "    print(f'\\tMoyenne pour les sujets malades = {round(np.mean(etendue_target_1),1)}')\n",
    "display_stats_etendue()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d72f86c",
   "metadata": {},
   "source": [
    "# Affichage de la qualité de  l'outil en fonction de la valeur de la caractéristique: l'étendue.\n",
    "## Au dessus de ce seuil, on considère que le sujet est malade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c083f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import make_interp_spline\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "valeurs_caracteristique = []\n",
    "erreur_caracteristique = []\n",
    "for seuil_etendue in range(120, 200, 10):\n",
    "    config = {'caracteristiques_a_utiliser': '3', 'seuil_3': seuil_etendue}\n",
    "    erreur = train(config, False)['erreur']\n",
    "    valeurs_caracteristique.append(seuil_etendue)\n",
    "    erreur_caracteristique.append(erreur)\n",
    "\n",
    "x_dense = np.linspace(min(valeurs_caracteristique), max(valeurs_caracteristique), 500)  # 500 points pour une courbe lisse\n",
    "spline = make_interp_spline(valeurs_caracteristique, erreur_caracteristique)\n",
    "y_dense = spline(x_dense)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(x_dense, y_dense, label='Erreur', color='b')\n",
    "plt.scatter(valeurs_caracteristique, erreur_caracteristique, color='r')\n",
    "plt.gca().tick_params(axis='y', which='major', labelsize=20) \n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1, decimals=1))\n",
    "plt.xticks(fontsize=20)\n",
    "plt.xlabel(\"Etendue\", fontsize=20)\n",
    "plt.ylabel('Erreur', fontsize=20)\n",
    "plt.xlim(min(valeurs_caracteristique), max(valeurs_caracteristique))\n",
    "plt.title(\"Erreur (%) en fonction de l'étendue\", fontsize=20)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d5922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l'étudiant devra remplacer le 100 ci dessous\n",
    "\n",
    "seuil_etendue = 100\n",
    "\n",
    "config = {'caracteristiques_a_utiliser': '3', 'seuil_3': seuil_etendue}\n",
    "metrics = train(config, False)\n",
    "print(f\"Erreur obtenue: {round(100*metrics['erreur'],1)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6815e02",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border-width:0; color:black; background-color:black\">\n",
    "<span style=\"font-size: 48px;\">2ème partie</span>\n",
    "<hr style=\"height:2px; border-width:0; color:black; background-color:black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f9685c",
   "metadata": {},
   "source": [
    "# Faire constater par l'étudiant que l'écart type observé pour les sujets malades est en moyenne plus élevé que l'écart type observé pour les sujets sains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b52e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_stats_std_dev():\n",
    "    std_dev_target_0 =[]\n",
    "    srd_dev_target_1 =[]\n",
    "    for id in id_to_fhr_last_hour.keys():\n",
    "        std_dev = compute_id_std_dev_last_hour(id)\n",
    "        if 0 == id_to_target[id]:\n",
    "            std_dev_target_0.append(std_dev)\n",
    "        else:\n",
    "            srd_dev_target_1.append(std_dev)\n",
    "    print(f'Ecart type:')\n",
    "    print(f'\\tMoyenne pour les sujets sains = {round(np.mean(std_dev_target_0),1)}')\n",
    "    print(f'\\tMoyenne pour les sujets malades = {round(np.mean(srd_dev_target_1),1)}')\n",
    "display_stats_std_dev()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1bacdc",
   "metadata": {},
   "source": [
    "# Affichage de la qualité de  l'outil en fonction du seuil d'écart type choisi pour identifier les sujets malades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba74d84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valeurs_caracteristique = []\n",
    "erreur_caracteristique = []\n",
    "for threshold_ecart_type in range(0,40+1,1):\n",
    "    config = {'caracteristiques_a_utiliser': '1', 'seuil_1': threshold_ecart_type}\n",
    "    erreur = train(config, False)['erreur']\n",
    "    valeurs_caracteristique.append(threshold_ecart_type)\n",
    "    erreur_caracteristique.append(erreur)\n",
    "\n",
    "x_dense = np.linspace(min(valeurs_caracteristique), max(valeurs_caracteristique), 500)  # 500 points pour une courbe lisse\n",
    "spline = make_interp_spline(valeurs_caracteristique, erreur_caracteristique)\n",
    "y_dense = spline(x_dense)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(x_dense, y_dense, label='Erreur', color='b')\n",
    "plt.scatter(valeurs_caracteristique, erreur_caracteristique, color='r')\n",
    "plt.gca().tick_params(axis='y', which='major', labelsize=20) \n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.xticks(fontsize=20)\n",
    "plt.xlabel(\"Ecart type\", fontsize=20)\n",
    "plt.ylabel('Erreur', fontsize=20)\n",
    "plt.title(\"Erreur en fonction du seuil de l'écart type\", fontsize=20)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e16f96a",
   "metadata": {},
   "source": [
    "# On demande à l'étudiant de choisir le seuil pour l'écart type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fedb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l'étudiant devra remplacer le 10 ci dessous\n",
    "seuil_ecart_type = 10\n",
    "\n",
    "config = {'caracteristiques_a_utiliser': '1', 'seuil_1': seuil_ecart_type}\n",
    "metrics = train(config, False)\n",
    "print(f\"Erreur obtenue: {round(100*metrics['erreur'],1)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fe9696",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border-width:0; color:black; background-color:black\">\n",
    "<span style=\"font-size: 48px;\">3ème partie</span>\n",
    "<hr style=\"height:2px; border-width:0; color:black; background-color:black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99169c8",
   "metadata": {},
   "source": [
    "# On combine les 2 caractéristiques précédentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e460a90",
   "metadata": {},
   "source": [
    "# PRIVE: Affichage de graphiques montrant les sujets sains et les sujets malades en fonction de 2 caractéristiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be28d33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pourcentage de points dans l'intervalle [moyenne-32.8, moyenne+32.8]\n",
    "# ce 32.8 a été choisi par une hyper parameter search \n",
    "@functools.lru_cache(maxsize=None)\n",
    "def proportion_around_mean_last_hour(id: int) -> float:\n",
    "    moyenne = compute_id_mean_last_hour(id)\n",
    "    delta = 32.8\n",
    "    return 100*proportion_in_interval_last_hour(id, moyenne-delta, moyenne+delta)\n",
    "\n",
    "\n",
    "def display_plot(x_method, x_method_name:str, y_method, y_method_name:str):\n",
    "    label0_coordinates = []\n",
    "    label1_coordinates = []\n",
    "    for id in id_to_fhr_last_hour.keys():\n",
    "        x =x_method(id)\n",
    "        y =y_method(id)\n",
    "        if id_to_target[id] == 0:\n",
    "            label0_coordinates.append((x,y))\n",
    "        else:\n",
    "            label1_coordinates.append((x,y))\n",
    "    x_label0, y_label0 = zip(*label0_coordinates)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.scatter(x_label0, y_label0, color='green', label='Sains')\n",
    "    x_label1, y_label1 = zip(*label1_coordinates)\n",
    "    plt.scatter(x_label1, y_label1, color='red', label='Malades')\n",
    "    #plt.title('Sujets sains et malades')\n",
    "    plt.xlabel(x_method_name)\n",
    "    plt.ylabel(y_method_name)\n",
    "    # Adding a legend to distinguish the series\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "caracteristiques = [(compute_id_std_dev_last_hour, \"Ecart type\"), \n",
    "                    (compute_id_range_last_hour, \"Etendue\"),\n",
    "                    #(compute_id_interquartile_range_last_hour, \"Ecart interquartile\"),\n",
    "                    #(proportion_around_mean_last_hour, \"% autour de la moyenne\"),\n",
    "                   ]\n",
    "\n",
    "for (x_method, x_method_name) in caracteristiques:\n",
    "    for (y_method, y_method_name) in caracteristiques:\n",
    "        if x_method_name == y_method_name:\n",
    "            continue            \n",
    "        display_plot(x_method, x_method_name, y_method, y_method_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8772350",
   "metadata": {},
   "source": [
    "# On demande à l'étudiant de choisir la valeur de ces 2 caractéristiques\n",
    "## On considère qu'un sujet est malade si l'étendue est supérieure au seuil indiqué ou si l'écart type est supérieur au seuil associé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddd8371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l'étudiant devra remplacer les 2 valeurs ci dessous\n",
    "\n",
    "seuil_ecart_type = 10\n",
    "seuil_etendue = 100\n",
    "\n",
    "\n",
    "config = {'caracteristiques_a_utiliser': '1+3', 'seuil_1': seuil_ecart_type, 'seuil_3': seuil_etendue,'label_si_resultats_differents': 1}\n",
    "metrics = train(config, False)\n",
    "print(f\"Erreur obtenue: {round(100*metrics['erreur'],1)}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
